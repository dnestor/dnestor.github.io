[
  {
    "path": "posts/2022-02-27-text-as-data-post-2a/",
    "title": "Text as Data Post 2A",
    "description": "Fixes to file sorting and methodology update",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-27",
    "categories": [
      "Text as Data"
    ],
    "contents": "\n\nContents\nFixes from Last Post\nText Extraction\nMethodology Update\nConclusion\n\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(knitr)\nlibrary(distill)\nlibrary(readtext)\nlibrary(tidyverse)\n\n\n\nFixes from Last Post\nAfter some additional testing and attempts to start extracting text and pre-processing, we noticed that some contracts were missing and others were corrupt and unreadable. This implies that our previous effort to sort files by machine-readability did not work as expected. Before moving on to creating a corpus, we must correct this error.\nWe start by creating a new loop to sort our documents. This loop differs from the previous post in several key ways:\nIn the initial for loop, we are now using the seq_along() function rather than length(). This helps avoid potential issues when the loop indexes rows so we can ensure all rows are included;\nWe’ve added a print() function into the loop to check the loop’s progress. Aside from adding a nice heuristic, this addition is essential as it will give us an idea of where the loop gets tripped up, thus showing which file we should inspect for corruption/other issues;\nDue to some issues with the pdftools package, we are now using the readtext library and function to extract text from each file. This problem was later resolved, but we decided to leave the function in to get some experience with additional packages. Note that the verbosity = 0 argument was used to reduce the number of errors printed for each file;\nWe changed the threshold function used to determine if a file was machine-readable from str_detect() to nchar() paired with a logical operator. In the original loop, the str_detect() function ended up evaluating to TRUE even if a minimal amount of text was present. By using nchar() < 100 as threshold criteria for moving a file, we are able to exclude any documents that clearly have too little text to be meaningful.\n\n\n# Change read function, add progress metric, change match criteria\nfor (i in seq_along(dest_file)){\n  print(i)\n  if(nchar(\n     readtext(\n       dest_file[i], verbosity = 0)$text) < 100){ \n        file_move(\n          dest_file[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n# Remove corrupted PDFs\ndest_file <- dest_file[-492]\ndest_file <- dest_file[-1507]\ndest_file <- dest_file[-1551]\ndest_file <- dest_file[-c(2475,2476,2477)]\ndest_file <- dest_file[-2380]\ndest_file <- dest_file[-c(2491,2492)]\ndest_file <- dest_file[-c(2494,2495)]\ndest_file <- dest_file[-2544]\n\n\n\nAfter running our new code, we find that 13 files are corrupt and remove them from the corpus, leaving 2,610 remaining. Of those files, 1,625 are not machine-readable, leaving only 985 in our corpus. Given the large number of documents removed, it is likely that we will have to add an OCR step to this project to ensure an appropriate sample size.\nText Extraction\nNow that we have our files sorted, we can move on to extracting text from each document so we can begin pre-processing.\nInitial Extraction\nTo extract text, we must first start by creating a new list of machine-readable files to use for our corpus. We use two different arrangements of the list.files() function to extract the file path and unique contract number of each document. These vectors later will be used in our loop to locate our target documents and add their unique identifiers to our data frame.\nWe must also figure out how to create a single string of text for each document as readtext() returns a list with each text box in the document representing an entry. After some testing, we find that the paste() function with sep = ’ ’ and collapse = ’ ’ easily accomplishes this task.\nFinally, we construct an empty dataframe and loop in the document text, unique identifier, and number of pages (which will become important in the next step).\n\n\n# Create list of all machine readable files\ncorpus_files <- list.files(\"/Users/dananestor/DACSS/Text as Data/Contract files\", \n                           pattern = \"pdf$\", full.names = TRUE)\ncorpus_file_names <- list.files(\"/Users/dananestor/DACSS/Text as Data/Contract files\", \n                                pattern = \"pdf$\") %>%\n                    str_remove_all(\".pdf\")\n\n# Test extraction, reduce vector elements to one for each contract\ncontract_vector_test <- \n  paste(\n    readtext(corpus_files[1])[2], \n  sep = '', collapse = '')\n\n# Create empty data frame \ncontract_frame <- tibble(\"a\",\"b\",\"c\")\ncolnames(contract_frame) <- c(\"contract\", \"text\", \"pages\")\n\n# Loop for adding contract number, document text, and page numbers\nfor (i in seq_along(corpus_files)){\n  print(i)\n  contract_frame[i,1] <- corpus_file_names[i]\n  contract_frame[i,2] <- \n    paste(\n      readtext(\n        corpus_files[i])[2], \n    sep = '', collapse = '')\n  contract_frame[i,3] <- pdf_info(corpus_files[i])$pages\n}\nhead(contract_frame)\n\n\n\n\n# A tibble: 6 × 3\n  contract text                                                  pages\n     <dbl> <chr>                                                 <dbl>\n1     1073 \"Teamsters - Kansas City\\n\\n\\n\\n\\n                  …    17\n2     1092 \"BLS Contract Collection – Metadata Header\\n\\n      …   127\n3     1100 \"Springfield, Missouri                              …    18\n4     1156 \"143764 Master Agrmnt2012-COVERS^^ut1 12/2/11 2:44 P…    47\n5     1223 \"MEMORANDUM OF AGREEMENT\\n                          …     1\n6     1224 \"MEMORANDUM OF AGREEMENT\\n                          …     2\n\nOnce the loop is complete - it takes a few minutes as the text extraction is somewhat computationally expensive given the size of the documents - we examine the new data frame and find it has the expected number of observations (985), columns (3), and that the data all appears in the correct location.\nFurther Readability Cleanup\nWhile examining our new dataframe of text strings and meta-data we notice that a number of rows contain what appears to be no text, but when printed show a pattern of escaped characters “\\n”. Further inspection shows these rows to be documents with more than 100 pages and that the “\\n” pattern repeats exactly (number of pages) - 1 times. We assume that this is a remnant of the readtext() function, where “\\n” is inserted if a page contains no readable characters. Because this subset of documents had more than 100 pages, the pattern repeated more than 100 times and thus was not caught initially by our nchar() < 100 filter.\nAs a workaround for this issue, we capitalize on the pattern outlined above and filter all observations where the number of characters is equal to (number of pages) - 1. The file path for each row is then added to a vector, and those files are moved in with the other documents that need to be OCR’ed.\n\n\n# Remove more non-OCR'ed documents\ncontract_frame_test <- contract_frame %>%\n  filter(nchar(contract_frame$text) == pages - 1)\ncontract_frame_rm <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                    contract_frame_test$contract, \".pdf\")\nfile_move(contract_frame_rm, \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")\n\n\n# Remove rows for non-OCR'ed documents\ncontract_frame <- contract_frame %>%\n  filter(nchar(contract_frame$text) != pages - 1)\n\n\n\nThis process yielded 288 additional non-readable files and our corpus now stands at 697 documents.\nMeta-data\nWhile we already added the unique contract number and number of pages to our dataframe, there are still a number of meta-data variables that we are missing. These variables offer rich information about our documents - such as employer and union names, industry, location, and number of workers - that will be useful in the future.\nBefore moving on to corpus creation, we first take a moment to add these meta-data variables via a left-join. A left-join indexes two dataframes via specified matching criteria, allowing for a mapping of rows across two frames with different structures. In our case, we used the left_join() function and matching criteria specifying that the unique identifier in each dataset should be used for the index (contract in the corpus_files object and CBA.File in the clean_contracts_table object).\nOnce a row from the first frame (in our case, the slimmed down corpus_files object) matches a row in the second frame (the master clean_contracts_table object of all files we initially downloaded), any columns from the second frame that do not exist in the first are appended to that first frame. This leaves one dataframe with all variables but only for the desired rows specified in that first frame.\n\n\n# Convert unique ID to integer for join \ncontract_frame$contract <- as.integer(contract_frame$contract)\n\n# Pull in meta-data using join, remove duplicate columns, filter for only employers w/2+ contracts\ncontract_frame <- \n  left_join(contract_frame, \n            clean_contracts_table, \n            by = c(\"contract\" = \"CBA.File\")) %>%\n  select(\n    c(contract, Employer.Name, Union, Location, Expiration.Date, NAICS., X.Wrkrs, pages, text)) \n\n\n\nOnce the left join is complete, we visually inspect it to ensure consistency of data (in this case, matching the employer and union from the text string to the Employer.Name and Union variables). Then, we re-arrange the columns using the select() function to remove duplicate variables and create a more orderly table.\n\n# A tibble: 6 × 9\n  contract Employer.Name Union Location Expiration.Date NAICS. X.Wrkrs\n     <dbl> <chr>         <chr> <chr>    <chr>            <dbl>   <dbl>\n1     1073 BUILDERS' AS… TEAM… MO       Mar 31, 2014     23621    2000\n2     1092 GENERAL & CO… UBC … OR, SW … May 31, 2006     23621    5000\n3     1100 BUILDERS' AS… ENGI… MO SPRI… Mar 31, 2007     23621    3200\n4     1156 AGC, WA       ENGI… WI       May 31, 2014     23731    2000\n5     1223 BUILDERS' AS… UBC … PA       Apr 30, 2015     23621   10000\n6     1224 BUILDERS' AS… CJA … PA       Apr 30, 2012     23621   10000\n# … with 2 more variables: pages <dbl>, text <chr>\n\nMethodology Update\nAfter further consideration and consultation about our research question and available data, we decided that a change was needed in our original methodological plan. One initial concern with this project was possible bias introduced into the data via organizations with more than one contract in the corpus being over-represented. This could skew our findings by giving greater weight to the language used by those organizations.\nAn additional avenue for potential co-linearity was suggested by Prof. Rice. He pointed out that, because substantial portions of these contracts could be similar (regardless of the number of submissions by the employer), our model may end up picking up that similarity and miss the more subtle latent topics that we are trying to detect. This could be resolved by subsetting documents that are substantially similar into categories, then comparing change within each category.\nOne final concern was in looking at changes to the corpus over time. Because time-series analysis implies a natural ordering that leads to correlation between observations, we needed to find a way to de-correlate our documents to ensure an accurate model. Prof. Song suggested an excellent work-around where, instead of looking at changes across the corpus and over the period of time covered by the corpus, we should pick a key case and look at contracts in-effect before and after to identify potential changes. We decided to move forward with this modification.\nTaken together, it becomes clear that further organization and subsetting of our data is necessary to avoid validity issues down the line. To accomplish this, we can harness a quirk of the data we already identified - multiple submissions from individual employers. We an filter out any documents that do not fit this pattern, and then analyze how the contracts for each employer changed before and after our key case. This eliminates any correlation or collinearity within the data, and has the added benefit of making our model more interpretable as it tracks change at the firm- rather than corpus-level.\n\n\ncontract_frame <- contract_frame %>%\n  group_by(Employer.Name) %>%\n  filter(n() >= 2)\n\n\n\nAfter grouping and filtering, we find that our corpus now stands at 186 documents.\nGrouping Considerations\nOne decision considered during this process was whether to group documents only by the employer name, or to create a new variable that combines the employer and union for a more fine-grained analysis at the negotiator level. The thinking was that the former could give insight into how an employer thinks about labor contracts, while the latter could give greater context to the results of negotiations between specific parties.\nBecause the research question focuses on employer motivations and the coarser grouping left us with so few documents, we decided to go with the former option. However, if we get more documents OCR’ed and added to the corpus, we would like to perform analysis at both levels to see if/how things change. We also feel that this could be a potential method for cross-validation: if the same firm, while negotiating with different unions in different contexts, insists on an addition to all contracts, this implies some level of corporate strategy when it comes to labor contracts.\nConclusion\nAfter running the updated loop initially, it became clear that an iterative process would be necessary as there were a number of files that were either corrupt or so large that they appeared corrupt in our code output. This was quite labor intensive as it required resetting and re-running a slow function (the loop itself) after each corrupt file was identified which took a significant amount of time. In the future, it would be best to update this part of the methodology with a more powerful loop that cuts out some of these steps.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-13T20:26:21-04:00",
    "input_file": "text-as-data-post-2a.knit.md"
  },
  {
    "path": "posts/2022-03-10-text-as-data-post-2/",
    "title": "Text as Data Post 2",
    "description": "Scraping union contract data",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-20",
    "categories": [
      "Text as Data",
      "Web Scraping"
    ],
    "contents": "\n\nContents\nSet Up\nTesting\nScraping Algorithm\nBibliography\n\n\n\nShow code\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(fs)\n\n\n\nSet Up\nToday we continue our project on union contract language by generating a set of data to turn into a corpus for analysis. This begins with identifying appropriate contracts to include. We then proceed to scrape these contracts, test them to see which have machine readable text, and sort those that do not into a separate folder for further processing.\nTo begin, an index is sourced from the US Department of Labor Office of Labor-Management Standards (OLMS) via their Online Public Disclosure Room. All 3,730 records are exported to a .CSV file, which is then read-in and filtered to include only private sector contracts. Given our research question, we are focusing on only private sector contracts as the public sector is not governed by the NLRA or overseen by the NLRB.\nScreen grab of OLMS Online Public Disclosure Room (OPDR) database of collective bargaining agreements \\[@opdr\\].\n\nShow code\n\n# Create index of of available CBAs; read in table downloaded from DOL source\nmessy_contracts_table <- \n  read.csv(\"/Users/dananestor/DACSS/Blog/Untitled/docs/_posts/2022-03-10-text-as-data-post-2/CBAList (2).csv\")\n\n# Remove contracts in public sector\nclean_contracts_table <- messy_contracts_table %>%\n  filter(Type != \"PUBLIC\")\n\n# NOT DONE: remove records missing industry or number of workers\n# clean_contracts_table <- clean_contracts_table %>%\n  #filter(NAICS. != is.na(NAICS.)) %>%\n  #filter(X.Wrkrs != is.na(X.Wrkrs))\n\n\n\nAfter filtering out public sector agreements, the number of records is lowered to 2,623. We considered removing any files that did not have meta-data available for industry or number of workers covered. However, this would eliminate approximately 800 records (nearly 30%) which is not insubstantial, so we decided against this. If this data is indeed necessary later we can attempt to impute it.\nTesting\nGiven the large number of files to be pulled, it makes sense to do some testing before attempting a full-scale scrape of this database. Our final algorithm will include sections to download, test for machine readability, and sort contract files so we can split it into these three elements to test.\nWe commence testing by subsetting our data. The .csv file we sourced from OLMS contains a variable for “CBA number” which turns out to be a unique identifier for each file. Additionally, the structure of the OPDR website utilizes a URL for each contract .PDF that only varies by this “CBA number” - thus, we can combine that variable with the URL stem to create a vector of URLs for our desired contracts.\nDownload\n\n\nShow code\n\n# Subset docs to test scrape\nclean_test_set <- clean_contracts_table[1:100,]\n\n# Create vectors for source file URL and destination\nurl_test <- paste0(\"https://olmsapps.dol.gov/olpdr/GetAttachmentServlet?docId=\", \n                   clean_test_set$CBA.File)\ndest_file_test <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                         clean_test_set$CBA.File, \".pdf\")\n\n# NOT RUN: Download pdfs from DOL\n# download.file(url_test, dest_file_test, method = 'libcurl', quiet = TRUE)\n\n\n\nOnce the data is subsetted into the first 100 contracts, we can create vectors for our test URLs and destination files based on the “CBA number” pattern. We then use the download.file() function - note the use of method = ‘libcurl’ which was essential in getting this process to work. This allows R to access “https://” sites and supports the simultaneous downloads provided by our vectors. Additionally, we set quiet = TRUE to suppress the number of messages as they can overwhelm RStudio.\nAfter testing, the first 100 files are successfully downloaded. At this point, we comment out the download.file function to prevent it from interfering as we iterate through the development of this post.\nMachine Readability\nCritically, we must ensure that the contracts we are downloading actually contain text for analysis. We turn to the pdftools library to accomplish this. Here, the pdf_text() function extracts character strings, returning a vector for each page in the document. These vectors can then be passed to the str_detect() function from the stringr package, which takes the RegEx input for one or more letters to return a new logical vector telling us whether (TRUE) or not (FALSE) any page of each contract contains machine readable text.\nWe can test how this combination of functions handles our data by testing it on files of known quality. To do this, we hand-selected three files - one newer contract formatted with text, one contract that had been run through an optical character recognition program (OCR), and one that was not machine readable.\n\n\nShow code\n\n# Test string detection on file known to contain text\ntest_obj_positive <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/2447.pdf\")\nhead(str_detect(test_obj_positive, \"[:alpha:]+\"))\n\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\nShow code\n\n# Now on file known not to have text\ntest_obj_negative <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/OCR/689.pdf\")\nhead(str_detect(test_obj_negative, \"[:alpha:]+\"))\n\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\n\nShow code\n\n# Now on file showing odd characteristics\ntest_obj_marginal <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/1803.pdf\")\nhead(str_detect(test_obj_marginal, \"[:alpha:]+\"))\n\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\nAfter running our functions on each document, we can see that it was able to correctly pick up the two .PDFs with text and identify the one without. Note that, for simplicity of output on the blog, the head() function has been applied. Initial explorations examined the entire vector returned.\nSorting Loop\nThe final piece of our algorithm is a loop that applies the previous test and automatically moves any files without machine readable text to a separate folder for further processing. This is accomplished with an if loop nested within a for loop - the for loop iterates through each file and the if loop moves the file should it fail the readability test. The essential function of this loop is accomplished with the file_move() function from the fs library.\n\n\nShow code\n\nfor (i in 1:length(dest_file_test)){\n  if(!(TRUE %in% #Test if any page in doc has one or more letter\n        str_detect(\n          pdf_text(dest_file_test[i]), \n          \"[:alpha:]+\"))){ \n        file_move( #If not, move to separate folder\n          dest_file_test[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n\n\nWhile the loop looks a bit convoluted, it is relatively simple. The combination of pdf_text() and str_detect() previously discussed provides a logical vector noting whether any page in each document contains one or more letter. This vector is then evaluated by the “TRUE %in%” phrase to determine whether the logical TRUE is present. Because this phrase is preceded by ! (and the whole argument is wrapped in parentheses), if TRUE is not present, the file_move() function is engaged and the contract is moved into our OCR folder for further processing.\nTesting shows that the loop is able to detect files without text, and successfully moves them to the desired folder.\nScraping Algorithm\nNow we can put all three elements together into our final algorithm. While testing, we noticed that the OLMS website will time out during the download phase if all files are not pulled within 60 seconds. To overcome this, we used a loop to split the download phase into 27 iterations of ~100 files each. Note that this includes a print() function as a progress heuristic so we can monitor which iteration we are on. Our for/if loop described earlier caps off the algorithm by removing any files without text.\n\n\nShow code\n\n# Create vector of contracts and destinations\nurl <- paste0(\"https://olmsapps.dol.gov/olpdr/GetAttachmentServlet?docId=\", \n              clean_contracts_table$CBA.File)\ndest_file <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                    clean_contracts_table$CBA.File, \".pdf\")\n\n# Segment to avoid timing out\ncuts <- cbind(seq(1,2623,100),seq(100,2700,100))\ncuts[27,2] = length(dest_file) #manually setting stop\n\n# Download files\nfor (i in 1:(length(cuts)/2)){\n  print(i)\n  download.file(\n    url[cuts[i,1]:cuts[i,2]], \n    dest_file[cuts[i,1]:cuts[i,2]], \n    method = 'libcurl', \n    quiet = TRUE)\n}\n\n# Test for text and move if OCR needed\nfor (i in 1:length(dest_file)){\n  if(!(TRUE %in%\n        str_detect(\n          pdf_text(dest_file[i]), \n          \"[:alpha:]+\"))){ \n        file_move(\n          dest_file[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n\n\nResults\nUsing this algorithm, 2,621 contracts were successfully downloaded, totaling 6.75 GB of data. Of those, 371 (984 MB) were not machine readable and require further processing. Two files, CBA numbers 297 and 2212, were too large to download as part of the loop so were pulled down manually. Results were cross validated by examining the local folder and testing a number of randomly-selected files.\nBibliography\nOLMS Online Public Disclosure Room. Retrieved from https://olmsapps.dol.gov/olpdr/?&_ga=2.149185590.1424332960.1649032027-354341244.1643300869#CBA%20Search/CBA%20Search/\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-03T22:29:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-05-text1/",
    "title": "Text as Data Post 1",
    "description": "General outline of my proposed research topic with analysis on feasability and a brief literature review.",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-05",
    "categories": [
      "Text as Data"
    ],
    "contents": "\nResearch Questions\nBackground\nManagement rights clauses are used to strategically reserve certain bargaining positions allowing “exclusive rights to manage, to direct…employees; to evaluate performance, to discipline and discharge employees, to adopt and enforce rules and regulations and policies.” Aided by friendly judicial decisions, these clauses are often beyond reach of the National Labor Relations Board (NLRB) and lower court review.\nBy claiming certain rights and bargaining to impasse, management is able to immediately implement their last best offer at the expiration of a collective bargaining agreement (CBA). Fighting this is a time consuming and expensive proposition, making this a significant point of leverage in contract negotiations.\nAdditionally, courts have relied on empirical analyses of the use of management rights clauses in some of their foundational decisions on this topic (see NLRB v. American National Insurance Co., 343 U.S. 395 (1952)). This opens the door for further analysis to not only improve understanding of the use and spread of these clauses, but even to introduce new evidence that may sway the judiciary towards a new view of the legitimacy of the practice.\nPotential Avenues of Exploration\nMap proliferation of specific clauses: how have these clauses spread? Is there a distinguishable network that we can identify from the data?\nHarness metadata of CBAs to create networks, attach directionality and weights based on identified management rights features to identify probabilities that specific clauses or contract language disseminated via a network or by chance.\n\nQuantify evolution of clauses: how has the language changed? Can we identify specific trends by sector, industry, size of organization, etc.?\nUse statistical analysis (machine learning classification models) to quantify the probability that a clause is related to management rights, combine with content/thematic analysis and/or critical discourse analysis to quantify these changes.\nEstablish supervised scaling approach to identify potential latent dimensions of the text and any correlations to networks, industries, etc.\n\nCompare against history: can we see correlations between changes in language or speed of proliferation against major milestones in the development of labor law and/or union strategy?\nData Sources\nCBA data is available online for contracts dating all the way back to 1935, 12 years before the Taft-Hartley amendment mandated centralized record keeping. The U.S. Department of Labor Office of Labor-Management Standards (DOL-OLMS) maintains mostly current (though also some historical) records both online and as a Microsoft Access database, including metadata on the bargaining parties, contract dates, employee counts, industry, and links to PDF copies of the full CBA. This dataset is inclusive of both public and private sector CBAs. As of February 5th, 2022, the DOL-OLMS database contained 3,730 entries.\nAdditionally, Cornell University’s School of Industrial and Labor Relations, Catherwood Library, maintains a historical database on behalf of DOL-OLMS. This is where most pre-1990 CBAs can be found, and it contains similar information to the DOL-OLMS database in terms of metadata and full CBAs. While Cornell is in the process of fully converting those files into machine-readable format, it is unclear what percentage of the collection is currently in this form - this could present a significant issue in data collection. That said, the data set currently contains 2,834 documents dating back to 1935.\nFinally, the University of California Berkeley’s Institute for Research on Labor and Employment maintains a fully-text-recognized database of union contracts from around the world. However, this data set is the smallest of the three and contains mostly public sector agreements, which differ substantially from the private sector when it comes to management rights clauses due to structural differences in the industry and laws governing these contracts. This database likely will not benefit the project.\nLiterature Review\nAsh, E., MacLeod, W. B., & Naidu, S. (n.d.-a). Optimal Contract Design in the Wild: Rigidity and Control in Collective Bargaining. 46.\nAnalysis of a corpus of 30,000 collective bargaining agreements from Canada from 1986 through 2015. Using ideas and methods from computational linguistics, authors extract measures of rigidity and worker control from the text of the contract clauses. They then analyze how rigidity and authority in contracts varies according to firm-level factors and external factors. This could be used to identify and externally validate the core methodology of this project.\n\nAsh, E., MacLeod, W. B., & Naidu, S. (n.d.-b). The Language of Contract: Promises and Power in Union Collective Bargaining Agreements. 59.\nSame authors and data as previous entry\n\nRosen, S. Z. (n.d.). Marceau, Drafting a Union Contract. Case Western Reserve Law Review, 6.\nThis handbook provides a perspective on the procedure of drafting a union contract in the 1960s. It could be helpful in identifying changes to procedure over time.\n\nWard, M. N. (2004). Contracting participation out of union culture: Patterns of modality and interactional moves in a labour contract settlement / Maurice Norman Ward. [Thesis, The University of Adelaide]. https://digital.library.adelaide.edu.au/dspace/handle/2440/22342\nThis doctoral thesis uses Systemic Functional Linguistics, Critical Discourse Analysis, qualitative, and computational analysis to investigate how language and power interact to construct relationships in the union setting and whether or not union discourse structures promote member participation. While it concentrates on only four documents, the methodologies described here could be useful for latent dimension identification and analysis.\n\n",
    "preview": {},
    "last_modified": "2022-03-10T18:27:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-02-networks2/",
    "title": "Short Assignment 2",
    "description": "Short assignment 2 for Political and Social Network Analysis",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-02",
    "categories": [],
    "contents": "\nUtilizing the provided bill cosponsorship data from the 112th congress and (modified) related import scripts, I read-in a CSV file and built both igraph and statnet objects to commence investigation. This data contains unique identifiers for each Member, the bill number, the date the Member joined in cosponsorship (and left, if applicable) and whether or not the member was an original co-sponsor.\n\n\ndata <- read_csv(\"~/DACSS/Network Analysis/govtrack_cosponsor_data_112_congress.csv\")\n\n# Provided script, modified so it would work with the available data\n nodes <- data[c(\"name\",\"thomas_id\",\"bioguide_id\",\"state\",\"district\")]\n  nodes <- distinct(nodes, name, state, bioguide_id, thomas_id, .keep_all = TRUE)\n  \n  #There are repeat entries for congress people who are given both a thomas_id (old system) and a\n  #bioguide_id (new system). Lets fix this by splitting and merging. \n    nodes_a  <- nodes[is.na(nodes$thomas_id),]\n    nodes_a  <- nodes[c(\"name\",\"state\",\"district\",\"bioguide_id\")]\n    nodes_b  <- nodes[is.na(nodes$bioguide_id),]\n    nodes_b  <- nodes[c(\"name\",\"state\",\"district\",\"thomas_id\")]\n    nodes    <- merge(x = nodes_a, y = nodes_b, by = c(\"name\",\"state\",\"district\"), all = TRUE)\n    rm(nodes_a);rm(nodes_b)\n  \n  #Lets also create a new ID that will be assigned to all congress people\n    nodes$ID <- 1:nrow(nodes)\n  \n  #Lets reorder the data putting the ID first\n    nodes <- nodes[c(\"ID\",\"name\",\"state\",\"district\",\"bioguide_id\",\"thomas_id\")]\n  \n#Now let's create a dataframe that contains just edge atributes\n  #Lets add the from_id collumn, replacing all the node attributes given for the senator cosponsoring\n    edge_list <- data\n    edge_list$node_1[!is.na(edge_list$thomas_id)]    <- nodes$ID[match(edge_list$thomas_id, nodes$thomas_id)][!is.na(edge_list$thomas_id)]\n    edge_list$node_1[!is.na(edge_list$bioguide_id)]  <- nodes$ID[match(edge_list$bioguide_id, nodes$bioguide_id)][!is.na(edge_list$bioguide_id)]\n    edge_list <- edge_list[c(\"node_1\",\"bill_number\",\"original_cosponsor\",\"date_signed\",\"date_withdrawn\",\"sponsor\")]\n  \n  #At this point, the \"edges\" dataframe contains links between sponsors and bills. Instead we want want \n  #the edgelist to represent to links between legislators. \n  #Let's do that by replacing the bill number collumn with the ID of the bill's original sponsor\n    sponsor_key    <- edge_list[edge_list$sponsor == TRUE, c(\"node_1\",\"bill_number\")]\n    edge_list$node_2   <- sponsor_key$node_1[match(edge_list$bill_number, sponsor_key$bill_number)]\n    \n  #Lets reorder the dataframe, putting the edgelist in the first two collumns\n      edge_list <- edge_list[c('node_1', 'node_2', 'bill_number','sponsor', 'original_cosponsor', 'date_signed', 'date_withdrawn')]\n    \n  #We dont need to keep the looped connections that represent legislators sponsoring their own bills\n      edge_list <- edge_list[edge_list$sponsor == FALSE,]\n  \n  #We can now remove the sponsor collum\n      edge_list <- edge_list[c('node_1', 'node_2', 'bill_number','original_cosponsor', 'date_signed', 'date_withdrawn')]\n      \n  #And remove unessesary objects\n      rm(sponsor_key)\n\n#Now let's make an igraph object\n  network_igraph <- graph_from_data_frame(d = edge_list, directed = TRUE, vertices = nodes)\n  \n#Now lets create a statnet object\n  \n  network_statnet <- network(as.matrix(edge_list[1:2]), matrix.type = \"edgelist\", directed = TRUE)\n  \n  network_statnet%e%'bill_number'         <- as.character(edge_list$bill_number)\n  network_statnet%e%'original_cosponsor'  <- as.character(edge_list$original_cosponsor)\n  network_statnet%e%'date_signed'         <- as.character(edge_list$date_signed)\n  network_statnet%e%'date_withdrawn'      <- as.character(edge_list$date_withdrawn)\n  \n  network_statnet%v%'name'        <-as.character(nodes$name[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'state'       <-as.character(nodes$state[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'district'    <-as.character(nodes$district[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'bioguide_id' <-as.character(nodes$bioguide_id[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'thomas_id'   <-as.character(nodes$thomas_id[match(nodes$ID,network_statnet%v%'vertex.names')])\n  \n\n#Lets create properly named objects and delete unessesary ones\n  network_nodes <- nodes\n  network_edgelist <- edge_list\n  rm(nodes);rm(data);rm(edge_list)\n\n\n\nIn further examining the data, we can see that there are 550 vertices and 1.32863^{5} edges in the igraph network. Additional features include:\nFeature\nT/F?\nBipartite\nFALSE\nDirected\nTRUE\nWeighted\nFALSE\nComparing the igraph object to the statnet object, we can see that the same network features hold true.\nWe can now take a dyad census to get an initial understanding of the connections in our network.\n\n\nigraph::dyad.census(network_igraph)\n\n\n$mut\n[1] 26734\n\n$asym\n[1] 79395\n\n$null\n[1] 44846\n\nsna::dyad.census(network_statnet)\n\n\n       Mut  Asym  Null\n[1,] 16388 35138 99449\n\nFor some reason, the two network objects are returning different measurements with respect to the number of dyad types. This will require further exploration at a later point.\nNext, we examine triads using a census\n\n\nigraph::triad_census(network_igraph)\n\n\n [1] 8427962 5409952 7049451  427375 1236075  731252 1134244  520424\n [9]  673093   36051  245990  472578  271413  243218  507035  191987\n\nsna::triad.census(network_statnet)\n\n\n         003     012     102   021D    021U   021C    111D   111U\n[1,] 8427962 8202667 4256736 427375 1236075 731252 1134244 520424\n       030T  030C    201   120D   120U   120C    210    300\n[1,] 673093 36051 245990 472578 271413 243218 507035 191987\n\nDespite the odd results of the dyad census, the triad census does show identical measurements across the two network object types. To double check that all triads are counted, we can calculate the number of potential triads as (550 * 549 * 548)/6 = 2.75781^{7}. Then, we sum the number of triads in our census sum(igraph::triad_census(network_igraph)) = 2.75781^{7}. The numbers match so we know our function worked correctly.\nFinally, we can look at transitivity within the network to determine the proportion of complete triads in the network.\n\n\ntransitivity(network_igraph)\n\n\n[1] 0.6258881\n\ngtrans(network_statnet)\n\n\n[1] 0.5606059\n\nHere, we see that the transitivity calculatuons differ between the two different network objects. This is likely because the statnet function calculates transitivity slightly differently for directed networks and omits certain triads missing information. However, both functions return a relatively high transitivity score, which makes sense given that this network is specifically intended to involve significant leveraging of connections and wheeling-dealing which requires more transitivity.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-02T21:08:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to my new blog, where I'll be posting assignments from my master's program in data analytics and computational social science",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-01-29",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-11T00:53:11-05:00",
    "input_file": {}
  }
]
