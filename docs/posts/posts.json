[
  {
    "path": "posts/2022-03-10-text-as-data-post-2/",
    "title": "Text as Data Post 2",
    "description": "Scraping union contract data",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-20",
    "categories": [
      "Text as Data",
      "Web Scraping"
    ],
    "contents": "\n\nContents\nSet Up\nTesting\nScraping Algorithm\nBibliography\n\n\n\nShow code\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(fs)\n\n\n\nSet Up\nToday we continue our project on union contract language by generating a set of data to turn into a corpus for analysis. This begins with identifying appropriate contracts to include. We then proceed to scrape these contracts, test them to see which have machine readable text, and sort those that do not into a separate folder for further processing.\nTo begin, an index is sourced from the US Department of Labor Office of Labor-Management Standards (OLMS) via their Online Public Disclosure Room. All 3,730 records are exported to a .CSV file, which is then read-in and filtered to include only private sector contracts. Given our research question, we are focusing on only private sector contracts as the public sector is not governed by the NLRA or overseen by the NLRB.\nScreen grab of OLMS Online Public Disclosure Room (OPDR) database of collective bargaining agreements \\[@opdr\\].\n\nShow code\n\n# Create index of of available CBAs; read in table downloaded from DOL source\nmessy_contracts_table <- \n  read.csv(\"/Users/dananestor/DACSS/Blog/Untitled/docs/_posts/2022-03-10-text-as-data-post-2/CBAList (2).csv\")\n\n# Remove contracts in public sector\nclean_contracts_table <- messy_contracts_table %>%\n  filter(Type != \"PUBLIC\")\n\n# NOT DONE: remove records missing industry or number of workers\n# clean_contracts_table <- clean_contracts_table %>%\n  #filter(NAICS. != is.na(NAICS.)) %>%\n  #filter(X.Wrkrs != is.na(X.Wrkrs))\n\n\n\nAfter filtering out public sector agreements, the number of records is lowered to 2,623. We considered removing any files that did not have meta-data available for industry or number of workers covered. However, this would eliminate approximately 800 records (nearly 30%) which is not insubstantial, so we decided against this. If this data is indeed necessary later we can attempt to impute it.\nTesting\nGiven the large number of files to be pulled, it makes sense to do some testing before attempting a full-scale scrape of this database. Our final algorithm will include sections to download, test for machine readability, and sort contract files so we can split it into these three elements to test.\nWe commence testing by subsetting our data. The .csv file we sourced from OLMS contains a variable for “CBA number” which turns out to be a unique identifier for each file. Additionally, the structure of the OPDR website utilizes a URL for each contract .PDF that only varies by this “CBA number” - thus, we can combine that variable with the URL stem to create a vector of URLs for our desired contracts.\nDownload\n\n\nShow code\n\n# Subset docs to test scrape\nclean_test_set <- clean_contracts_table[1:100,]\n\n# Create vectors for source file URL and destination\nurl_test <- paste0(\"https://olmsapps.dol.gov/olpdr/GetAttachmentServlet?docId=\", \n                   clean_test_set$CBA.File)\ndest_file_test <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                         clean_test_set$CBA.File, \".pdf\")\n\n# NOT RUN: Download pdfs from DOL\n# download.file(url_test, dest_file_test, method = 'libcurl', quiet = TRUE)\n\n\n\nOnce the data is subsetted into the first 100 contracts, we can create vectors for our test URLs and destination files based on the “CBA number” pattern. We then use the download.file() function - note the use of method = ‘libcurl’ which was essential in getting this process to work. This allows R to access “https://” sites and supports the simultaneous downloads provided by our vectors. Additionally, we set quiet = TRUE to suppress the number of messages as they can overwhelm RStudio.\nAfter testing, the first 100 files are successfully downloaded. At this point, we comment out the download.file function to prevent it from interfering as we iterate through the development of this post.\nMachine Readability\nCritically, we must ensure that the contracts we are downloading actually contain text for analysis. We turn to the pdftools library to accomplish this. Here, the pdf_text() function extracts character strings, returning a vector for each page in the document. These vectors can then be passed to the str_detect() function from the stringr package, which takes the RegEx input for one or more letters to return a new logical vector telling us whether (TRUE) or not (FALSE) any page of each contract contains machine readable text.\nWe can test how this combination of functions handles our data by testing it on files of known quality. To do this, we hand-selected three files - one newer contract formatted with text, one contract that had been run through an optical character recognition program (OCR), and one that was not machine readable.\n\n\nShow code\n\n# Test string detection on file known to contain text\ntest_obj_positive <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/2447.pdf\")\nhead(str_detect(test_obj_positive, \"[:alpha:]+\"))\n\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\nShow code\n\n# Now on file known not to have text\ntest_obj_negative <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/OCR/689.pdf\")\nhead(str_detect(test_obj_negative, \"[:alpha:]+\"))\n\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\n\nShow code\n\n# Now on file showing odd characteristics\ntest_obj_marginal <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/1803.pdf\")\nhead(str_detect(test_obj_marginal, \"[:alpha:]+\"))\n\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\nAfter running our functions on each document, we can see that it was able to correctly pick up the two .PDFs with text and identify the one without. Note that, for simplicity of output on the blog, the head() function has been applied. Initial explorations examined the entire vector returned.\nSorting Loop\nThe final piece of our algorithm is a loop that applies the previous test and automatically moves any files without machine readable text to a separate folder for further processing. This is accomplished with an if loop nested within a for loop - the for loop iterates through each file and the if loop moves the file should it fail the readability test. The essential function of this loop is accomplished with the file_move() function from the fs library.\n\n\nShow code\n\nfor (i in 1:length(dest_file_test)){\n  if(!(TRUE %in% #Test if any page in doc has one or more letter\n        str_detect(\n          pdf_text(dest_file_test[i]), \n          \"[:alpha:]+\"))){ \n        file_move( #If not, move to separate folder\n          dest_file_test[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n\n\nWhile the loop looks a bit convoluted, it is relatively simple. The combination of pdf_text() and str_detect() previously discussed provides a logical vector noting whether any page in each document contains one or more letter. This vector is then evaluated by the “TRUE %in%” phrase to determine whether the logical TRUE is present. Because this phrase is preceded by ! (and the whole argument is wrapped in parentheses), if TRUE is not present, the file_move() function is engaged and the contract is moved into our OCR folder for further processing.\nTesting shows that the loop is able to detect files without text, and successfully moves them to the desired folder.\nScraping Algorithm\nNow we can put all three elements together into our final algorithm. While testing, we noticed that the OLMS website will time out during the download phase if all files are not pulled within 60 seconds. To overcome this, we used a loop to split the download phase into 27 iterations of ~100 files each. Note that this includes a print() function as a progress heuristic so we can monitor which iteration we are on. Our for/if loop described earlier caps off the algorithm by removing any files without text.\n\n\nShow code\n\n# Create vector of contracts and destinations\nurl <- paste0(\"https://olmsapps.dol.gov/olpdr/GetAttachmentServlet?docId=\", \n              clean_contracts_table$CBA.File)\ndest_file <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                    clean_contracts_table$CBA.File, \".pdf\")\n\n# Segment to avoid timing out\ncuts <- cbind(seq(1,2623,100),seq(100,2700,100))\ncuts[27,2] = length(dest_file) #manually setting stop\n\n# Download files\nfor (i in 1:(length(cuts)/2)){\n  print(i)\n  download.file(\n    url[cuts[i,1]:cuts[i,2]], \n    dest_file[cuts[i,1]:cuts[i,2]], \n    method = 'libcurl', \n    quiet = TRUE)\n}\n\n# Test for text and move if OCR needed\nfor (i in 1:length(dest_file)){\n  if(!(TRUE %in%\n        str_detect(\n          pdf_text(dest_file[i]), \n          \"[:alpha:]+\"))){ \n        file_move(\n          dest_file[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n\n\nResults\nUsing this algorithm, 2,621 contracts were successfully downloaded, totaling 6.75 GB of data. Of those, 371 (984 MB) were not machine readable and require further processing. Two files, CBA numbers 297 and 2212, were too large to download as part of the loop so were pulled down manually. Results were cross validated by examining the local folder and testing a number of randomly-selected files.\nBibliography\nOLMS Online Public Disclosure Room. Retrieved from https://olmsapps.dol.gov/olpdr/?&_ga=2.149185590.1424332960.1649032027-354341244.1643300869#CBA%20Search/CBA%20Search/\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-03T22:29:40-04:00",
    "input_file": "text-as-data-post-2.knit.md"
  },
  {
    "path": "posts/2022-02-05-text1/",
    "title": "Text as Data Post 1",
    "description": "General outline of my proposed research topic with analysis on feasability and a brief literature review.",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-05",
    "categories": [
      "Text as Data"
    ],
    "contents": "\nResearch Questions\nBackground\nManagement rights clauses are used to strategically reserve certain bargaining positions allowing “exclusive rights to manage, to direct…employees; to evaluate performance, to discipline and discharge employees, to adopt and enforce rules and regulations and policies.” Aided by friendly judicial decisions, these clauses are often beyond reach of the National Labor Relations Board (NLRB) and lower court review.\nBy claiming certain rights and bargaining to impasse, management is able to immediately implement their last best offer at the expiration of a collective bargaining agreement (CBA). Fighting this is a time consuming and expensive proposition, making this a significant point of leverage in contract negotiations.\nAdditionally, courts have relied on empirical analyses of the use of management rights clauses in some of their foundational decisions on this topic (see NLRB v. American National Insurance Co., 343 U.S. 395 (1952)). This opens the door for further analysis to not only improve understanding of the use and spread of these clauses, but even to introduce new evidence that may sway the judiciary towards a new view of the legitimacy of the practice.\nPotential Avenues of Exploration\nMap proliferation of specific clauses: how have these clauses spread? Is there a distinguishable network that we can identify from the data?\nHarness metadata of CBAs to create networks, attach directionality and weights based on identified management rights features to identify probabilities that specific clauses or contract language disseminated via a network or by chance.\n\nQuantify evolution of clauses: how has the language changed? Can we identify specific trends by sector, industry, size of organization, etc.?\nUse statistical analysis (machine learning classification models) to quantify the probability that a clause is related to management rights, combine with content/thematic analysis and/or critical discourse analysis to quantify these changes.\nEstablish supervised scaling approach to identify potential latent dimensions of the text and any correlations to networks, industries, etc.\n\nCompare against history: can we see correlations between changes in language or speed of proliferation against major milestones in the development of labor law and/or union strategy?\nData Sources\nCBA data is available online for contracts dating all the way back to 1935, 12 years before the Taft-Hartley amendment mandated centralized record keeping. The U.S. Department of Labor Office of Labor-Management Standards (DOL-OLMS) maintains mostly current (though also some historical) records both online and as a Microsoft Access database, including metadata on the bargaining parties, contract dates, employee counts, industry, and links to PDF copies of the full CBA. This dataset is inclusive of both public and private sector CBAs. As of February 5th, 2022, the DOL-OLMS database contained 3,730 entries.\nAdditionally, Cornell University’s School of Industrial and Labor Relations, Catherwood Library, maintains a historical database on behalf of DOL-OLMS. This is where most pre-1990 CBAs can be found, and it contains similar information to the DOL-OLMS database in terms of metadata and full CBAs. While Cornell is in the process of fully converting those files into machine-readable format, it is unclear what percentage of the collection is currently in this form - this could present a significant issue in data collection. That said, the data set currently contains 2,834 documents dating back to 1935.\nFinally, the University of California Berkeley’s Institute for Research on Labor and Employment maintains a fully-text-recognized database of union contracts from around the world. However, this data set is the smallest of the three and contains mostly public sector agreements, which differ substantially from the private sector when it comes to management rights clauses due to structural differences in the industry and laws governing these contracts. This database likely will not benefit the project.\nLiterature Review\nAsh, E., MacLeod, W. B., & Naidu, S. (n.d.-a). Optimal Contract Design in the Wild: Rigidity and Control in Collective Bargaining. 46.\nAnalysis of a corpus of 30,000 collective bargaining agreements from Canada from 1986 through 2015. Using ideas and methods from computational linguistics, authors extract measures of rigidity and worker control from the text of the contract clauses. They then analyze how rigidity and authority in contracts varies according to firm-level factors and external factors. This could be used to identify and externally validate the core methodology of this project.\n\nAsh, E., MacLeod, W. B., & Naidu, S. (n.d.-b). The Language of Contract: Promises and Power in Union Collective Bargaining Agreements. 59.\nSame authors and data as previous entry\n\nRosen, S. Z. (n.d.). Marceau, Drafting a Union Contract. Case Western Reserve Law Review, 6.\nThis handbook provides a perspective on the procedure of drafting a union contract in the 1960s. It could be helpful in identifying changes to procedure over time.\n\nWard, M. N. (2004). Contracting participation out of union culture: Patterns of modality and interactional moves in a labour contract settlement / Maurice Norman Ward. [Thesis, The University of Adelaide]. https://digital.library.adelaide.edu.au/dspace/handle/2440/22342\nThis doctoral thesis uses Systemic Functional Linguistics, Critical Discourse Analysis, qualitative, and computational analysis to investigate how language and power interact to construct relationships in the union setting and whether or not union discourse structures promote member participation. While it concentrates on only four documents, the methodologies described here could be useful for latent dimension identification and analysis.\n\n",
    "preview": {},
    "last_modified": "2022-03-10T18:27:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-02-networks2/",
    "title": "Short Assignment 2",
    "description": "Short assignment 2 for Political and Social Network Analysis",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-02",
    "categories": [],
    "contents": "\nUtilizing the provided bill cosponsorship data from the 112th congress and (modified) related import scripts, I read-in a CSV file and built both igraph and statnet objects to commence investigation. This data contains unique identifiers for each Member, the bill number, the date the Member joined in cosponsorship (and left, if applicable) and whether or not the member was an original co-sponsor.\n\n\ndata <- read_csv(\"~/DACSS/Network Analysis/govtrack_cosponsor_data_112_congress.csv\")\n\n# Provided script, modified so it would work with the available data\n nodes <- data[c(\"name\",\"thomas_id\",\"bioguide_id\",\"state\",\"district\")]\n  nodes <- distinct(nodes, name, state, bioguide_id, thomas_id, .keep_all = TRUE)\n  \n  #There are repeat entries for congress people who are given both a thomas_id (old system) and a\n  #bioguide_id (new system). Lets fix this by splitting and merging. \n    nodes_a  <- nodes[is.na(nodes$thomas_id),]\n    nodes_a  <- nodes[c(\"name\",\"state\",\"district\",\"bioguide_id\")]\n    nodes_b  <- nodes[is.na(nodes$bioguide_id),]\n    nodes_b  <- nodes[c(\"name\",\"state\",\"district\",\"thomas_id\")]\n    nodes    <- merge(x = nodes_a, y = nodes_b, by = c(\"name\",\"state\",\"district\"), all = TRUE)\n    rm(nodes_a);rm(nodes_b)\n  \n  #Lets also create a new ID that will be assigned to all congress people\n    nodes$ID <- 1:nrow(nodes)\n  \n  #Lets reorder the data putting the ID first\n    nodes <- nodes[c(\"ID\",\"name\",\"state\",\"district\",\"bioguide_id\",\"thomas_id\")]\n  \n#Now let's create a dataframe that contains just edge atributes\n  #Lets add the from_id collumn, replacing all the node attributes given for the senator cosponsoring\n    edge_list <- data\n    edge_list$node_1[!is.na(edge_list$thomas_id)]    <- nodes$ID[match(edge_list$thomas_id, nodes$thomas_id)][!is.na(edge_list$thomas_id)]\n    edge_list$node_1[!is.na(edge_list$bioguide_id)]  <- nodes$ID[match(edge_list$bioguide_id, nodes$bioguide_id)][!is.na(edge_list$bioguide_id)]\n    edge_list <- edge_list[c(\"node_1\",\"bill_number\",\"original_cosponsor\",\"date_signed\",\"date_withdrawn\",\"sponsor\")]\n  \n  #At this point, the \"edges\" dataframe contains links between sponsors and bills. Instead we want want \n  #the edgelist to represent to links between legislators. \n  #Let's do that by replacing the bill number collumn with the ID of the bill's original sponsor\n    sponsor_key    <- edge_list[edge_list$sponsor == TRUE, c(\"node_1\",\"bill_number\")]\n    edge_list$node_2   <- sponsor_key$node_1[match(edge_list$bill_number, sponsor_key$bill_number)]\n    \n  #Lets reorder the dataframe, putting the edgelist in the first two collumns\n      edge_list <- edge_list[c('node_1', 'node_2', 'bill_number','sponsor', 'original_cosponsor', 'date_signed', 'date_withdrawn')]\n    \n  #We dont need to keep the looped connections that represent legislators sponsoring their own bills\n      edge_list <- edge_list[edge_list$sponsor == FALSE,]\n  \n  #We can now remove the sponsor collum\n      edge_list <- edge_list[c('node_1', 'node_2', 'bill_number','original_cosponsor', 'date_signed', 'date_withdrawn')]\n      \n  #And remove unessesary objects\n      rm(sponsor_key)\n\n#Now let's make an igraph object\n  network_igraph <- graph_from_data_frame(d = edge_list, directed = TRUE, vertices = nodes)\n  \n#Now lets create a statnet object\n  \n  network_statnet <- network(as.matrix(edge_list[1:2]), matrix.type = \"edgelist\", directed = TRUE)\n  \n  network_statnet%e%'bill_number'         <- as.character(edge_list$bill_number)\n  network_statnet%e%'original_cosponsor'  <- as.character(edge_list$original_cosponsor)\n  network_statnet%e%'date_signed'         <- as.character(edge_list$date_signed)\n  network_statnet%e%'date_withdrawn'      <- as.character(edge_list$date_withdrawn)\n  \n  network_statnet%v%'name'        <-as.character(nodes$name[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'state'       <-as.character(nodes$state[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'district'    <-as.character(nodes$district[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'bioguide_id' <-as.character(nodes$bioguide_id[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'thomas_id'   <-as.character(nodes$thomas_id[match(nodes$ID,network_statnet%v%'vertex.names')])\n  \n\n#Lets create properly named objects and delete unessesary ones\n  network_nodes <- nodes\n  network_edgelist <- edge_list\n  rm(nodes);rm(data);rm(edge_list)\n\n\n\nIn further examining the data, we can see that there are 550 vertices and 1.32863^{5} edges in the igraph network. Additional features include:\nFeature\nT/F?\nBipartite\nFALSE\nDirected\nTRUE\nWeighted\nFALSE\nComparing the igraph object to the statnet object, we can see that the same network features hold true.\nWe can now take a dyad census to get an initial understanding of the connections in our network.\n\n\nigraph::dyad.census(network_igraph)\n\n\n$mut\n[1] 26734\n\n$asym\n[1] 79395\n\n$null\n[1] 44846\n\nsna::dyad.census(network_statnet)\n\n\n       Mut  Asym  Null\n[1,] 16388 35138 99449\n\nFor some reason, the two network objects are returning different measurements with respect to the number of dyad types. This will require further exploration at a later point.\nNext, we examine triads using a census\n\n\nigraph::triad_census(network_igraph)\n\n\n [1] 8427962 5409952 7049451  427375 1236075  731252 1134244  520424\n [9]  673093   36051  245990  472578  271413  243218  507035  191987\n\nsna::triad.census(network_statnet)\n\n\n         003     012     102   021D    021U   021C    111D   111U\n[1,] 8427962 8202667 4256736 427375 1236075 731252 1134244 520424\n       030T  030C    201   120D   120U   120C    210    300\n[1,] 673093 36051 245990 472578 271413 243218 507035 191987\n\nDespite the odd results of the dyad census, the triad census does show identical measurements across the two network object types. To double check that all triads are counted, we can calculate the number of potential triads as (550 * 549 * 548)/6 = 2.75781^{7}. Then, we sum the number of triads in our census sum(igraph::triad_census(network_igraph)) = 2.75781^{7}. The numbers match so we know our function worked correctly.\nFinally, we can look at transitivity within the network to determine the proportion of complete triads in the network.\n\n\ntransitivity(network_igraph)\n\n\n[1] 0.6258881\n\ngtrans(network_statnet)\n\n\n[1] 0.5606059\n\nHere, we see that the transitivity calculatuons differ between the two different network objects. This is likely because the statnet function calculates transitivity slightly differently for directed networks and omits certain triads missing information. However, both functions return a relatively high transitivity score, which makes sense given that this network is specifically intended to involve significant leveraging of connections and wheeling-dealing which requires more transitivity.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-02T21:08:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to my new blog, where I'll be posting assignments from my master's program in data analytics and computational social science",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-01-29",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-11T00:53:11-05:00",
    "input_file": {}
  }
]
