[
  {
    "path": "posts/2022-03-06-text-as-data-post-3/",
    "title": "Text as Data Post 3",
    "description": "Data exploration, corpus creation, and pre-processing",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-03-06",
    "categories": [
      "Text as Data",
      "Data Exploration",
      "Pre-processing"
    ],
    "contents": "\n\nContents\nData Exploration\nCorpus Creation\nPre-processing: from Corpus to DFM\n\n\n\nShow code\n\nknitr::opts_chunk$set(echo = TRUE)\n\nlibrary(knitr)\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(cowplot)\nlibrary(lubridate)\nlibrary(blscrapeR)\nlibrary(ggridges)\nlibrary(kableExtra)\n\n\n\n\n\nShow code\n\n# Read in data created in last post\nclean_contracts_table <- read_csv(\n  \"/Users/dananestor/DACSS/Blog/Untitled/docs/_posts/2022-03-06-text-as-data-post-3/clean_contracts_table.csv\",\n  show_col_types = FALSE)\ncontract_frame <- read_csv(\n  \"/Users/dananestor/DACSS/Blog/Untitled/docs/_posts/2022-03-06-text-as-data-post-3/contract_frame.csv\",\n  show_col_types = FALSE) %>%\n  group_by(Employer.Name)\n\n\n\nData Exploration\nNow that our files are sorted, text is extracted, and meta-data is gathered, we can proceed to explore the characteristics of the information that we seek to analyze. In this post, we will examine the key players in our inquiry - employers and unions - to understand how they are represented in our data. We will then walk through the creation of a corpus and pre-processing of text necessary to move on to modelling and further study.\nEmployers and Unions\nOne major assumption in the methodology discussed in Post 2A is a sufficient number of employers with contracts before and after our key case to provide a statistically significant sample - too few, and it is likely that the data will lack the subtlety necessary to detect the semantic changes at issue. We can investigate the effects that cleaning our data and imposing this methodology have had on sample size by examining the original data obtained from OLMS and our subset consisting of employers with more than two machine-readable contracts.\n\n\nShow code\n\n# Create table to show number of employers and unions\nstats_table <- tibble( \n  \"Rows\" = c(\n    \"Number of Employers\", \n    \"Number of Unions\"), \n  \"Subset\" = c(\n    n_distinct(contract_frame$Employer.Name), \n    n_distinct(contract_frame$Union)), \n  \"Original\" = c(\n    n_distinct(clean_contracts_table$Employer.Name), \n    n_distinct(clean_contracts_table$Union))) %>%\n  column_to_rownames(\"Rows\")\n\nkable(stats_table, \n      caption = 'Employers and Unions Across Data Sorting') %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                full_width = FALSE)\n\n\n\nTable 1: Employers and Unions Across Data Sorting\n\n\n\n\nSubset\n\n\nOriginal\n\n\nNumber of Employers\n\n\n74\n\n\n2022\n\n\nNumber of Unions\n\n\n100\n\n\n1409\n\n\nA quick check of the number of unique employers in the original data set vs. the subset shows a reduction from 2022 to 74, which is a 96% decrease. Unions show a similar drop off, though slightly lower at a reduction of 93%.\nWhile this is a concerning reduction, we need more information to determine the best course of action. Even a small number of employers can allow for a result as long as each has a sufficient number of contracts to detect changes in our measurements. Next, we pull out the top employers and unions as ranked by the number of contracts in our subset to see if this may be the case.\n\n\nShow code\n\n# Lists of top 10 employers and unions in subset\ntop_employers <- contract_frame %>% \n  summarise(n = n()) %>% \n  arrange(desc(n)) %>%\n  slice_head(n = 10)\n\nkable(top_employers, \n      caption = 'Top 10 Employers in Subset') %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTable 2: Top 10 Employers in Subset\n\n\nEmployer.Name\n\n\nn\n\n\nBUILDERS’ ASSOCIATION OF MO\n\n\n8\n\n\nAGC, AK\n\n\n7\n\n\nAGC, WA\n\n\n5\n\n\nROADS BRIDGES & AIRPORTS\n\n\n5\n\n\nALPHA-OMEGA CHANGE ENGINEERING, INC (AOCE)\n\n\n4\n\n\nASSOCIATED BUILDING CONTRACTORS OF LAFAYETTE, INC (CEMENT MASONS)\n\n\n4\n\n\nCALIFORNIA PROCESSORS INC.\n\n\n4\n\n\nTRAINING, REHABILITATION AND DEVELOPMENT INSTITUTE INC.\n\n\n4\n\n\nAGC, MN (HVY-HWY-RR)\n\n\n3\n\n\nAMERICAN NATIONAL INSURANCE COMPANY\n\n\n3\n\n\nWhile not ideal, we can see that there are eight employers with four or more contracts in the subset, meaning that they could have two before and after the key case.\n\n\nShow code\n\ntop_unions <- contract_frame %>% \n  ungroup() %>%\n  group_by(Union) %>%\n  summarise(n = n()) %>% \n  arrange(desc(n)) %>%\n  slice_head(n = 10)\n\ntop_unions$Union <- str_trunc(top_unions$Union, 40)\n\nkable(top_unions, \n      caption = 'Top 10 Unions in Subset') %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"), \n                full_width = FALSE)\n\n\n\nTable 3: Top 10 Unions in Subset\n\n\nUnion\n\n\nn\n\n\nPLASTERERS AND CEMENT MASONS AFL-CIO …\n\n\n23\n\n\nENGINEERS, OPERATING, AFL-CIO (IUOE) …\n\n\n10\n\n\nOPCMIA LOCAL 692\n\n\n6\n\n\nBOILERMAKERS AFL-CIO (BBF) LODGE 104\n\n\n4\n\n\nELECTRICAL WORKERS IBEW AFL-CIO (IBEW…\n\n\n3\n\n\nENGINEERS, OPERATING, AFL-CIO (IUOE) …\n\n\n3\n\n\nIRON WORKERS AFL-CIO (BSOIW) LOCAL UN…\n\n\n3\n\n\nLABORERS (LIUNA) LOCAL UNION 110/LAB…\n\n\n3\n\n\nMACHINISTS AFL-CIO (IAM) LODGE 2003\n\n\n3\n\n\nPLASTERERS AND CEMENT MASONS AFL-CIO …\n\n\n3\n\n\nUnions in the subset offer fewer overall individuals with four or more contracts, but two of them have 10 or more which could be useful for a very specific dive into changes over time.\nOne notable finding when examining the top employers and unions is the lack of consistency in naming conventions. For example, the trade association AGC appears multiple times in the top 10 entries of employers, with each instance representing a chapter in a different region. Similarly, in the union data, we see different entries for local chapters of different unions, such as the International Union of Operating Engineers. However, it is unclear whether this is a positive or negative characteristic.\nOn the one hand, combining regional entities into one organization may allow for higher sensitivity in detecting topics given the small sample size if the assumption is made that strategy related to federal labor law will be coordinated centrally. Alternately, this may make it more difficult to detect (and possibly control for) regional variations in contracts motivated by state-level labor law. We must take this into account when modelling and developing an iterative analysis to critically understand how the judicial environment might influence labor contracts.\nExpiration Date Distribution\nTo pick a court case that would allow for the analysis proposed in this project, we can look at the distribution of expiration dates to see at minimum what range we should consider. Hopefully we will find a multi-modal distribution that would clearly indicate the farthest back to look, given the assumption that contracts expiring before a case will not be influenced by it.\nNote that this rests on a somewhat weak assumption that contracts expiring after the case we pick will have been drafted after the case was decided. This is obviously not guaranteed. However, because we do not have meta-data available on the effective date of the contract, we will have to use this variable as our best approximation. This is an area for methodological improvement in the future.\nWe’ll start by plotting the number of contracts expiring per-year in our subset to see if any patterns emerge.\n\n\nShow code\n\n# Format dates as M/D/Y\nclean_contracts_table$Expiration.Date <- mdy(clean_contracts_table$Expiration.Date) \ncontract_frame$Expiration.Date <- mdy(contract_frame$Expiration.Date)\n\n# Expiration date distro plots\noptions(scipen = 999) #suppress scientific notation\n\ndate_subset <- ggplot( #plot count by date in subset\n  subset(contract_frame, \n         !is.na(Expiration.Date)), \n  aes(Expiration.Date)) +\n  geom_step(stat = \"count\") +\n  scale_x_date(\n    name = \"Expiration Date\", \n    date_labels = \"%b-%y\", \n    date_breaks = \"4 year\")\n\nggsave(\"date_subset.png\", date_subset)\n\n\n\nSubsetExamining this chart, we can see that there appear to be spikes around 2012, 2014, 2015, and 2017. It also shows a number of contracts expiring before 2012, and that our data is limited when it comes to contracts expiring after 2018. Given this information, we may want to search for a case decided in 2011 or 2012 to ensure that we have enough data points on either side.\nThis plot also leads us to wonder if our original data set would offer a more informative distribution (thus supporting the work to further OCR documents). To check, we create the same plot but with our original data.\n\n\nShow code\n\ndate_original <- ggplot( #plot count by date in original\n  subset(clean_contracts_table, \n         !is.na(Expiration.Date)), \n  aes(Expiration.Date)) +\n  geom_step(stat = \"count\") +\n  scale_x_date(\n    name = \"Expiration Date\", \n    date_labels = \"%b-%y\", \n    date_breaks = \"4 year\", \n    limit = c(as.Date(\"2000-01-01\"), #reduce x-axis for interpretability\n              as.Date(\"2028-01-01\"))) \n\nggsave(\"date_original.png\", date_original)\n\n\n\nOriginalHere we see a very different distribution - unsurprisingly, there are many more observations in each peak. But the range has also expanded to include many more contemporary documents. Moreover, we can see that the largest peak on the subset graph - occurring in 2012 - is quite a bit smaller than the largest peaks in our original data. And, as we had initially hoped, this distribution appears bi-modal, showing a large proportion of contracts expiring before 2010 and after 2018, which could provide a very interesting gap to investigate for a court case.\nTaken together, the evidence mounts for increasing our sample size. However, in the absence of time, we are at least on somewhat stable ground in looking for cases around 2011 or 2012.\nThis data only shows us a distribution relative to the entire corpus, however, and since we are proposing an analysis across employers to try and control for covariance, we need to tie our visualizations to employers specifically. This way, we can see where each employer falls in terms of when their contracts represented in our corpus expire. Again, ideally this would show a multi-modal distribution for at least a few employers so we can be sure that our analysis will incorporate data representative of our research question.\nTo proceed, we graph the distribution of expiration dates within our subset, but grouped by employer.\n\n\nShow code\n\ndate_grouped_subset <- ggplot( # plot distro by employer\n  contract_frame, \n  aes(x = Expiration.Date, \n      y = Employer.Name, \n      fill = Employer.Name)) +\n  stat_binline(bins = 28, scale = 0.8, draw_baseline = F) +\n  theme_ridges() + \n  theme(legend.position = \"none\", \n        axis.text.y = element_blank()) +\n  scale_x_date(\n    name = \"Expiration Date\", \n    date_labels = \"%b-%y\", \n    date_breaks = \"4 year\")\n\nggsave(\"date_grouped_subset.png\", date_grouped_subset)\n\n\n\nGroupedThis graph is less informative than we had hoped. While it does show spacing between contracts for each employer, it is difficult to visualize the density of contracts for each employer over time. While we tried to accomplish this with the geom_density_ridges function, it was not sensitive enough to pick up on the multimodality of the data.\nHowever, we can glean from this chart that most employers do in fact have contracts that expired before and after 2012, further supporting picking a case from that timeframe.\nGeographic Distribution\nWe can also look at the distribution of contracts by state and region to see any potential representation issues in our contracts. To do this, we clean up the location column and add in regions using a matching function and data from the census bureau; then plot.\n\n\nShow code\n\n# Regions distro\ncontract_frame$Loc_unique <- contract_frame$Location %>% #clean up locations\n  str_replace_all(\"OHIO\", \"OH\") %>%\n  str_replace_all(\"FORT HILL, OK\", \"OK\") %>%\n  str_replace_all(\"PASCAGONOULA, MISSISSIPPI\", \"MS\") %>%\n  str_replace_all(\"PASCAGOULA\", \"MS\") %>%\n  str_replace_all(\"U.S.\", \"NATIONAL\")  %>%\n  str_replace_all(\"(..).+\", '\\\\1') %>%\n  str_replace_all(\"NA\", \"NATIONAL\")\n\nregions <- as_tibble( #add region column using census data\n  read_csv('/Users/dananestor/DACSS/Useful Data/us census bureau regions and divisions.csv')) %>%\n  rbind(rep(\"NATIONAL\", 4))\ncontract_frame$Region <- regions$Region[match(\n                            contract_frame$Loc_unique, \n                            regions$`State Code`)] %>%\n  str_replace_all(\"NATIONAL\", \"National\")\n\nstates_subset <- contract_frame %>% #plot by state\n  ungroup() %>%\n  ggplot(aes(Loc_unique)) +\n  geom_bar(stat = \"count\") +\n  labs(\n    title = \"Distribution by State\", \n    x = \"State\") +\n  theme(\n    axis.text.x = \n      element_text(angle = 45, hjust=1))\n\nregions_subset <- contract_frame %>% #plot by region\n  ungroup() %>%\n  ggplot(aes(Region)) +\n  geom_bar(stat = \"count\") +\n  labs(\n    title = \"Distribution by Region\", \n    x = \"Region\") +\n  theme(\n    axis.text.x = \n      element_text(angle = 45, \n                   hjust=1))\n\nregions_duplex <- plot_grid(states_subset, \n          regions_subset, \n          rel_widths = c(2.25,.75))\n\nggsave(\"regions_duplex.png\", regions_duplex)\n\n\n\nStates and RegionsIn examining the results, we can see that Alaska, Indiana, and Missouri have much higher numbers of contracts relative to other states than their populations would suggest. Similarly, the regions plot shows that the Northeast seems to be underrepresented relative to the other regions. This should be taken into account in any analysis, and we may need to control for it as we continue data exploration.\nIndustry Distribution\nFinally, we look at distribution across industry within our subset. One of the papers we are relying on for the formulation of this methodology - Ash, et. al. (Ash, MacLeod, and Naidu 2019) - found significant results when comparing their findings across industries so it is important not only to have this information available, but to see how it is represented in our data.\nWe start by importing NAICS data from the census bureau, which will allow us to match against the NAICS meta-data already in our contracts file. We then match the two columns and pull in the full industry name, cleaning up along the way. However, this is more data than we need - it gives hundreds of sub-industries, which would make a graph hard to read and near meaningless. We can fix this by pulling the first two digits of the NAICS code and matching them to a list of top-level industry groupings, allowing for a look at the distribution across major economic sectors.\n\n\nShow code\n\n# Industry distro\nNAICS_short <- as_tibble( #add industry info\n                  read_csv(\"/Users/dananestor/DACSS/Useful Data/NAICS_2_digits.csv\", \n                            show_col_types = FALSE)) \ncontract_frame$Industry <- naics$industry_title[match(\n                            contract_frame$NAICS., \n                            naics$industry_code)] %>%\n  str_replace_all(\"NAICS\\\\s\\\\d+\\\\s(.+)\", \"\\\\1\") %>%\n  str_replace_all(\"NAICS\\\\d+\\\\s\\\\d+\\\\s(.+)\", \"\\\\1\")\ncontract_frame <- contract_frame %>%\n  mutate(\"Short_Industry\" = \n           substr(NAICS., 1, 2))\ncontract_frame$Short_Industry <- NAICS_short$Definition[match(\n                                    contract_frame$Short_Industry, \n                                    NAICS_short$Sector)]\n\nlabel_data <- contract_frame %>%  #set up for polar graph labels\n  ungroup() %>%\n  count(Short_Industry) %>%\n  cbind(\"index\" = 1:10)\nlabel_data$Short_Industry <- str_replace_na(label_data$Short_Industry)\nnumber_of_bar <- nrow(label_data)\nangle <-  90 - 360 * (label_data$index-0.5) / number_of_bar #calc angle for each\nlabel_data$hjust <- ifelse(angle < -90, #calc side of graph\n                           1, \n                           0) \nlabel_data$angle <- ifelse(angle < -90, #flip angle to make readable\n                           angle + 180, \n                           angle) \n\nindustry_subset <- label_data %>% #plot by industry\n  ungroup() %>%\n  ggplot(aes(x = as.factor(index), \n             y = n)) +\n  geom_bar(stat = \"identity\", \n           fill = alpha(\"black\", 0.7)) +\n  ylim(-50, 220) +\n  theme_minimal() + #remove themes, labels\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid = element_blank(),\n    plot.margin = unit(rep(-2,4), \"cm\") #Adjust the margin to make in sort labels are not truncated\n  ) +\n  coord_polar(start = 0) + #change to polar coords\n  geom_text(aes(x = index, \n                y = n, \n                label = Short_Industry, \n                hjust = hjust), \n            color = \"black\", \n            fontface = \"bold\",\n            alpha = 0.6, \n            size = 2.5, \n            angle = label_data$angle, \n            inherit.aes = FALSE, \n            nudge_y = 5) \n\nggsave(\"industry_subset.png\", industry_subset)\n\n\n\nIndustryAfter adding more industry information to our data, we chart the count of contracts in our subset in each sector using a circular barplot (h/t to the R Graph Gallery, (Holtz, n.d.)). This shows what we would expect based on subject area knowledge - the construction and manufacturing industries are heavily represented, while most others have only a few contracts in our subset. This may be useful information as we move through analysis.\nCorpus Creation\nNow that we understand our data better, it is time to start processing the contract text so we can start analysis. We begin with the creation of a corpus. Using the corpus() function in the quanteda package, we are able to easily extract the text from our data frame and convert it into a useable corpus object. We also use the summary() function to create a corpus summary object and explore the dimensions of our text data.\n\n\nShow code\n\ncontract_corpus <- corpus(contract_frame$text)\ncontract_corpus_summary <- summary(contract_corpus, n = Inf)\n\nkable(summary(contract_corpus_summary), \n      caption = 'Dimensions of Corpus') %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"), \n                full_width = FALSE)\n\n\n\nTable 4: Dimensions of Corpus\n\n\n\n\nText\n\n\nTypes\n\n\nTokens\n\n\nSentences\n\n\n\n\nLength:186\n\n\nMin. : 32\n\n\nMin. : 36\n\n\nMin. : 1.0\n\n\n\n\nClass :character\n\n\n1st Qu.: 1688\n\n\n1st Qu.: 9648\n\n\n1st Qu.: 316.2\n\n\n\n\nMode :character\n\n\nMedian : 2340\n\n\nMedian : 15649\n\n\nMedian : 472.5\n\n\n\n\nNA\n\n\nMean : 2877\n\n\nMean : 26596\n\n\nMean : 789.4\n\n\n\n\nNA\n\n\n3rd Qu.: 3668\n\n\n3rd Qu.: 35453\n\n\n3rd Qu.: 956.5\n\n\n\n\nNA\n\n\nMax. :14163\n\n\nMax. :202218\n\n\nMax. :4006.0\n\n\nWhile this mostly looks good, it is notable that the minimum for tokens and sentences is concerningly low - it would be shocking if a labor contract were only 1 sentence or 36 words. We will have to explore these documents further to ensure that they are appropriate for inclusion and were converted properly.\nPre-processing: from Corpus to DFM\nWe finally have a corpus to work with! But to utilize it appropriately, we must also create a document feature matrix (DFM) to facilitate further analysis.\nWhen creating this DFM, we must make some pre-processing decisions to help reduce dimensionality that only contains noise and ensure that grammatical quirks do not hinder our analysis. This includes removing English stopwords and punctuation, and converting all characters to lower case. In so doing, we are filtering out certain parts of our text string that do not provide contextual information, and may cause certain forms of the same word to be treated separately (such as a capitalized version at the start of a sentence). We also decide to exclude words that are used rarely (less than three times) as these likely will not contribute to our topic modelling given their infrequent appearance in the data.\nHowever, these decisions may have a major effect on the output of our model, especially given the rigorous structure of legal documents. One potential issue may arise when further subsetting text strings to focus on certain paragraphs or clauses, like a management rights section. Thus, we must keep in mind the effects of this pre-processing as we develop our analytical techniques.\n\n\nShow code\n\ncontract_dfm <- contract_corpus %>%\n   dfm(remove = stopwords('english'), \n      remove_punct = TRUE, \n      tolower = TRUE) %>%\n  dfm_trim(min_termfreq = 3, \n           verbose = FALSE)\n\ncontract_dfm_trimmed <- contract_dfm %>%\n  dfm_remove(pattern = c(\"$\", \n                         \"^\", \n                         regex(\"[:digit:]\")))\n\npng(\"subset_wordcloud_initial.png\", res = 300)\ntextplot_wordcloud(contract_dfm_trimmed, \n                   scale = c(7, .4), \n                   max_words = 500)\n\ndev.off()\n\n\n\nOnce our DFM is created, we can check out how we did with a word cloud.\nWordcloudThis is exciting! Though it does highlight a few issues. Namely: - The most-frequent word is “shall” which, in our context, likely has little meaning so we will have to determine which “legalese” stopwords to remove in addition to those found in the “English” library. - Individual numbers (and letters) are still present despite our trying to remove them from the DFM; these likely correspond to section headings and thus are meaningless at the moment - we must determine how to remove them - We might consider lemmatizing given the different forms of similar words present (based, base, bases & make, made)\nBut the major point is we now have our data in a format we can use and a good idea of the timeframe for our key case! While we have some additional pre-processing to take care of, hopefully soon we will be able to move on to topic modelling.\n\n\n\nAsh, Elliott, W Bentley MacLeod, and Suresh Naidu. 2019. “The Language of Contract: Promises and Power in Union Collective Bargaining Agreements,” March, 59.\n\n\nHoltz, Yan. n.d. “Circular Barplot | the R Graph Gallery.” https://r-graph-gallery.com/circular-barplot.html.\n\n\n\n\n",
    "preview": "https://dnestor.github.io/posts/2022-03-06-text-as-data-post-3/subset_wordcloud_initial.png",
    "last_modified": "2022-04-19T17:41:22-04:00",
    "input_file": "text-as-data-post-3.knit.md"
  },
  {
    "path": "posts/2022-02-27-text-as-data-post-2a/",
    "title": "Text as Data Post 2A",
    "description": "Fixes to file sorting and methodology update",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-27",
    "categories": [
      "Text as Data"
    ],
    "contents": "\n\nContents\nFixes from Last Post\nText Extraction\nMethodology Update\nConclusion\n\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(knitr)\nlibrary(distill)\nlibrary(readtext)\nlibrary(tidyverse)\n\n\n\nFixes from Last Post\nAfter some additional testing and attempts to start extracting text and pre-processing, we noticed that some contracts were missing and others were corrupt and unreadable. This implies that our previous effort to sort files by machine-readability did not work as expected. Before moving on to creating a corpus, we must correct this error.\nWe start by creating a new loop to sort our documents. This loop differs from the previous post in several key ways:\nIn the initial for loop, we are now using the seq_along() function rather than length(). This helps avoid potential issues when the loop indexes rows so we can ensure all rows are included;\nWe’ve added a print() function into the loop to check the loop’s progress. Aside from adding a nice heuristic, this addition is essential as it will give us an idea of where the loop gets tripped up, thus showing which file we should inspect for corruption/other issues;\nDue to some issues with the pdftools package, we are now using the readtext library and function to extract text from each file. This problem was later resolved, but we decided to leave the function in to get some experience with additional packages. Note that the verbosity = 0 argument was used to reduce the number of errors printed for each file;\nWe changed the threshold function used to determine if a file was machine-readable from str_detect() to nchar() paired with a logical operator. In the original loop, the str_detect() function ended up evaluating to TRUE even if a minimal amount of text was present. By using nchar() < 100 as threshold criteria for moving a file, we are able to exclude any documents that clearly have too little text to be meaningful.\n\n\n# Change read function, add progress metric, change match criteria\nfor (i in seq_along(dest_file)){\n  print(i)\n  if(nchar(\n     readtext(\n       dest_file[i], verbosity = 0)$text) < 100){ \n        file_move(\n          dest_file[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n# Remove corrupted PDFs\ndest_file <- dest_file[-492]\ndest_file <- dest_file[-1507]\ndest_file <- dest_file[-1551]\ndest_file <- dest_file[-c(2475,2476,2477)]\ndest_file <- dest_file[-2380]\ndest_file <- dest_file[-c(2491,2492)]\ndest_file <- dest_file[-c(2494,2495)]\ndest_file <- dest_file[-2544]\n\n\n\nAfter running our new code, we find that 13 files are corrupt and remove them from the corpus, leaving 2,610 remaining. Of those files, 1,625 are not machine-readable, leaving only 985 in our corpus. Given the large number of documents removed, it is likely that we will have to add an OCR step to this project to ensure an appropriate sample size.\nText Extraction\nNow that we have our files sorted, we can move on to extracting text from each document so we can begin pre-processing.\nInitial Extraction\nTo extract text, we must first start by creating a new list of machine-readable files to use for our corpus. We use two different arrangements of the list.files() function to extract the file path and unique contract number of each document. These vectors later will be used in our loop to locate our target documents and add their unique identifiers to our data frame.\nWe must also figure out how to create a single string of text for each document as readtext() returns a list with each text box in the document representing an entry. After some testing, we find that the paste() function with sep = ’ ’ and collapse = ’ ’ easily accomplishes this task.\nFinally, we construct an empty dataframe and loop in the document text, unique identifier, and number of pages (which will become important in the next step).\n\n\n# Create list of all machine readable files\ncorpus_files <- list.files(\"/Users/dananestor/DACSS/Text as Data/Contract files\", \n                           pattern = \"pdf$\", full.names = TRUE)\ncorpus_file_names <- list.files(\"/Users/dananestor/DACSS/Text as Data/Contract files\", \n                                pattern = \"pdf$\") %>%\n                    str_remove_all(\".pdf\")\n\n# Test extraction, reduce vector elements to one for each contract\ncontract_vector_test <- \n  paste(\n    readtext(corpus_files[1])[2], \n  sep = '', collapse = '')\n\n# Create empty data frame \ncontract_frame <- tibble(\"a\",\"b\",\"c\")\ncolnames(contract_frame) <- c(\"contract\", \"text\", \"pages\")\n\n# Loop for adding contract number, document text, and page numbers\nfor (i in seq_along(corpus_files)){\n  print(i)\n  contract_frame[i,1] <- corpus_file_names[i]\n  contract_frame[i,2] <- \n    paste(\n      readtext(\n        corpus_files[i])[2], \n    sep = '', collapse = '')\n  contract_frame[i,3] <- pdf_info(corpus_files[i])$pages\n}\nhead(contract_frame)\n\n\n\n\n# A tibble: 6 × 3\n  contract text                                                  pages\n     <dbl> <chr>                                                 <dbl>\n1     1073 \"Teamsters - Kansas City\\n\\n\\n\\n\\n                  …    17\n2     1092 \"BLS Contract Collection – Metadata Header\\n\\n      …   127\n3     1100 \"Springfield, Missouri                              …    18\n4     1156 \"143764 Master Agrmnt2012-COVERS^^ut1 12/2/11 2:44 P…    47\n5     1223 \"MEMORANDUM OF AGREEMENT\\n                          …     1\n6     1224 \"MEMORANDUM OF AGREEMENT\\n                          …     2\n\nOnce the loop is complete - it takes a few minutes as the text extraction is somewhat computationally expensive given the size of the documents - we examine the new data frame and find it has the expected number of observations (985), columns (3), and that the data all appears in the correct location.\nFurther Readability Cleanup\nWhile examining our new dataframe of text strings and meta-data we notice that a number of rows contain what appears to be no text, but when printed show a pattern of escaped characters “\\n”. Further inspection shows these rows to be documents with more than 100 pages and that the “\\n” pattern repeats exactly (number of pages) - 1 times. We assume that this is a remnant of the readtext() function, where “\\n” is inserted if a page contains no readable characters. Because this subset of documents had more than 100 pages, the pattern repeated more than 100 times and thus was not caught initially by our nchar() < 100 filter.\nAs a workaround for this issue, we capitalize on the pattern outlined above and filter all observations where the number of characters is equal to (number of pages) - 1. The file path for each row is then added to a vector, and those files are moved in with the other documents that need to be OCR’ed.\n\n\n# Remove more non-OCR'ed documents\ncontract_frame_test <- contract_frame %>%\n  filter(nchar(contract_frame$text) == pages - 1)\ncontract_frame_rm <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                    contract_frame_test$contract, \".pdf\")\nfile_move(contract_frame_rm, \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")\n\n\n# Remove rows for non-OCR'ed documents\ncontract_frame <- contract_frame %>%\n  filter(nchar(contract_frame$text) != pages - 1)\n\n\n\nThis process yielded 288 additional non-readable files and our corpus now stands at 697 documents.\nMeta-data\nWhile we already added the unique contract number and number of pages to our dataframe, there are still a number of meta-data variables that we are missing. These variables offer rich information about our documents - such as employer and union names, industry, location, and number of workers - that will be useful in the future.\nBefore moving on to corpus creation, we first take a moment to add these meta-data variables via a left-join. A left-join indexes two dataframes via specified matching criteria, allowing for a mapping of rows across two frames with different structures. In our case, we used the left_join() function and matching criteria specifying that the unique identifier in each dataset should be used for the index (contract in the corpus_files object and CBA.File in the clean_contracts_table object).\nOnce a row from the first frame (in our case, the slimmed down corpus_files object) matches a row in the second frame (the master clean_contracts_table object of all files we initially downloaded), any columns from the second frame that do not exist in the first are appended to that first frame. This leaves one dataframe with all variables but only for the desired rows specified in that first frame.\n\n\n# Convert unique ID to integer for join \ncontract_frame$contract <- as.integer(contract_frame$contract)\n\n# Pull in meta-data using join, remove duplicate columns, filter for only employers w/2+ contracts\ncontract_frame <- \n  left_join(contract_frame, \n            clean_contracts_table, \n            by = c(\"contract\" = \"CBA.File\")) %>%\n  select(\n    c(contract, Employer.Name, Union, Location, Expiration.Date, NAICS., X.Wrkrs, pages, text)) \n\n\n\nOnce the left join is complete, we visually inspect it to ensure consistency of data (in this case, matching the employer and union from the text string to the Employer.Name and Union variables). Then, we re-arrange the columns using the select() function to remove duplicate variables and create a more orderly table.\n\n# A tibble: 6 × 9\n  contract Employer.Name Union Location Expiration.Date NAICS. X.Wrkrs\n     <dbl> <chr>         <chr> <chr>    <chr>            <dbl>   <dbl>\n1     1073 BUILDERS' AS… TEAM… MO       Mar 31, 2014     23621    2000\n2     1092 GENERAL & CO… UBC … OR, SW … May 31, 2006     23621    5000\n3     1100 BUILDERS' AS… ENGI… MO SPRI… Mar 31, 2007     23621    3200\n4     1156 AGC, WA       ENGI… WI       May 31, 2014     23731    2000\n5     1223 BUILDERS' AS… UBC … PA       Apr 30, 2015     23621   10000\n6     1224 BUILDERS' AS… CJA … PA       Apr 30, 2012     23621   10000\n# … with 2 more variables: pages <dbl>, text <chr>\n\nMethodology Update\nAfter further consideration and consultation about our research question and available data, we decided that a change was needed in our original methodological plan. One initial concern with this project was possible bias introduced into the data via organizations with more than one contract in the corpus being over-represented. This could skew our findings by giving greater weight to the language used by those organizations.\nAn additional avenue for potential co-linearity was suggested by Prof. Rice. He pointed out that, because substantial portions of these contracts could be similar (regardless of the number of submissions by the employer), our model may end up picking up that similarity and miss the more subtle latent topics that we are trying to detect. This could be resolved by subsetting documents that are substantially similar into categories, then comparing change within each category.\nOne final concern was in looking at changes to the corpus over time. Because time-series analysis implies a natural ordering that leads to correlation between observations, we needed to find a way to de-correlate our documents to ensure an accurate model. Prof. Song suggested an excellent work-around where, instead of looking at changes across the corpus and over the period of time covered by the corpus, we should pick a key case and look at contracts in-effect before and after to identify potential changes. We decided to move forward with this modification.\nTaken together, it becomes clear that further organization and subsetting of our data is necessary to avoid validity issues down the line. To accomplish this, we can harness a quirk of the data we already identified - multiple submissions from individual employers. We an filter out any documents that do not fit this pattern, and then analyze how the contracts for each employer changed before and after our key case. This eliminates any correlation or collinearity within the data, and has the added benefit of making our model more interpretable as it tracks change at the firm- rather than corpus-level.\n\n\ncontract_frame <- contract_frame %>%\n  group_by(Employer.Name) %>%\n  filter(n() >= 2)\n\n\n\nAfter grouping and filtering, we find that our corpus now stands at 186 documents.\nGrouping Considerations\nOne decision considered during this process was whether to group documents only by the employer name, or to create a new variable that combines the employer and union for a more fine-grained analysis at the negotiator level. The thinking was that the former could give insight into how an employer thinks about labor contracts, while the latter could give greater context to the results of negotiations between specific parties.\nBecause the research question focuses on employer motivations and the coarser grouping left us with so few documents, we decided to go with the former option. However, if we get more documents OCR’ed and added to the corpus, we would like to perform analysis at both levels to see if/how things change. We also feel that this could be a potential method for cross-validation: if the same firm, while negotiating with different unions in different contexts, insists on an addition to all contracts, this implies some level of corporate strategy when it comes to labor contracts.\nConclusion\nAfter running the updated loop initially, it became clear that an iterative process would be necessary as there were a number of files that were either corrupt or so large that they appeared corrupt in our code output. This was quite labor intensive as it required resetting and re-running a slow function (the loop itself) after each corrupt file was identified which took a significant amount of time. In the future, it would be best to update this part of the methodology with a more powerful loop that cuts out some of these steps.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-13T20:26:22-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-10-text-as-data-post-2/",
    "title": "Text as Data Post 2",
    "description": "Scraping union contract data",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-20",
    "categories": [
      "Text as Data",
      "Web Scraping"
    ],
    "contents": "\n\nContents\nSet Up\nTesting\nScraping Algorithm\nBibliography\n\n\n\nShow code\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(fs)\n\n\n\nSet Up\nToday we continue our project on union contract language by generating a set of data to turn into a corpus for analysis. This begins with identifying appropriate contracts to include. We then proceed to scrape these contracts, test them to see which have machine readable text, and sort those that do not into a separate folder for further processing.\nTo begin, an index is sourced from the US Department of Labor Office of Labor-Management Standards (OLMS) via their Online Public Disclosure Room. All 3,730 records are exported to a .CSV file, which is then read-in and filtered to include only private sector contracts. Given our research question, we are focusing on only private sector contracts as the public sector is not governed by the NLRA or overseen by the NLRB.\nScreen grab of OLMS Online Public Disclosure Room (OPDR) database of collective bargaining agreements \\[@opdr\\].\n\nShow code\n\n# Create index of of available CBAs; read in table downloaded from DOL source\nmessy_contracts_table <- \n  read.csv(\"/Users/dananestor/DACSS/Blog/Untitled/docs/_posts/2022-03-10-text-as-data-post-2/CBAList (2).csv\")\n\n# Remove contracts in public sector\nclean_contracts_table <- messy_contracts_table %>%\n  filter(Type != \"PUBLIC\")\n\n# NOT DONE: remove records missing industry or number of workers\n# clean_contracts_table <- clean_contracts_table %>%\n  #filter(NAICS. != is.na(NAICS.)) %>%\n  #filter(X.Wrkrs != is.na(X.Wrkrs))\n\n\n\nAfter filtering out public sector agreements, the number of records is lowered to 2,623. We considered removing any files that did not have meta-data available for industry or number of workers covered. However, this would eliminate approximately 800 records (nearly 30%) which is not insubstantial, so we decided against this. If this data is indeed necessary later we can attempt to impute it.\nTesting\nGiven the large number of files to be pulled, it makes sense to do some testing before attempting a full-scale scrape of this database. Our final algorithm will include sections to download, test for machine readability, and sort contract files so we can split it into these three elements to test.\nWe commence testing by subsetting our data. The .csv file we sourced from OLMS contains a variable for “CBA number” which turns out to be a unique identifier for each file. Additionally, the structure of the OPDR website utilizes a URL for each contract .PDF that only varies by this “CBA number” - thus, we can combine that variable with the URL stem to create a vector of URLs for our desired contracts.\nDownload\n\n\nShow code\n\n# Subset docs to test scrape\nclean_test_set <- clean_contracts_table[1:100,]\n\n# Create vectors for source file URL and destination\nurl_test <- paste0(\"https://olmsapps.dol.gov/olpdr/GetAttachmentServlet?docId=\", \n                   clean_test_set$CBA.File)\ndest_file_test <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                         clean_test_set$CBA.File, \".pdf\")\n\n# NOT RUN: Download pdfs from DOL\n# download.file(url_test, dest_file_test, method = 'libcurl', quiet = TRUE)\n\n\n\nOnce the data is subsetted into the first 100 contracts, we can create vectors for our test URLs and destination files based on the “CBA number” pattern. We then use the download.file() function - note the use of method = ‘libcurl’ which was essential in getting this process to work. This allows R to access “https://” sites and supports the simultaneous downloads provided by our vectors. Additionally, we set quiet = TRUE to suppress the number of messages as they can overwhelm RStudio.\nAfter testing, the first 100 files are successfully downloaded. At this point, we comment out the download.file function to prevent it from interfering as we iterate through the development of this post.\nMachine Readability\nCritically, we must ensure that the contracts we are downloading actually contain text for analysis. We turn to the pdftools library to accomplish this. Here, the pdf_text() function extracts character strings, returning a vector for each page in the document. These vectors can then be passed to the str_detect() function from the stringr package, which takes the RegEx input for one or more letters to return a new logical vector telling us whether (TRUE) or not (FALSE) any page of each contract contains machine readable text.\nWe can test how this combination of functions handles our data by testing it on files of known quality. To do this, we hand-selected three files - one newer contract formatted with text, one contract that had been run through an optical character recognition program (OCR), and one that was not machine readable.\n\n\nShow code\n\n# Test string detection on file known to contain text\ntest_obj_positive <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/2447.pdf\")\nhead(str_detect(test_obj_positive, \"[:alpha:]+\"))\n\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\nShow code\n\n# Now on file known not to have text\ntest_obj_negative <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/OCR/689.pdf\")\nhead(str_detect(test_obj_negative, \"[:alpha:]+\"))\n\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\n\nShow code\n\n# Now on file showing odd characteristics\ntest_obj_marginal <- pdf_text(\"/Users/dananestor/DACSS/Text as Data/Contract files/1803.pdf\")\nhead(str_detect(test_obj_marginal, \"[:alpha:]+\"))\n\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\nAfter running our functions on each document, we can see that it was able to correctly pick up the two .PDFs with text and identify the one without. Note that, for simplicity of output on the blog, the head() function has been applied. Initial explorations examined the entire vector returned.\nSorting Loop\nThe final piece of our algorithm is a loop that applies the previous test and automatically moves any files without machine readable text to a separate folder for further processing. This is accomplished with an if loop nested within a for loop - the for loop iterates through each file and the if loop moves the file should it fail the readability test. The essential function of this loop is accomplished with the file_move() function from the fs library.\n\n\nShow code\n\nfor (i in 1:length(dest_file_test)){\n  if(!(TRUE %in% #Test if any page in doc has one or more letter\n        str_detect(\n          pdf_text(dest_file_test[i]), \n          \"[:alpha:]+\"))){ \n        file_move( #If not, move to separate folder\n          dest_file_test[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n\n\nWhile the loop looks a bit convoluted, it is relatively simple. The combination of pdf_text() and str_detect() previously discussed provides a logical vector noting whether any page in each document contains one or more letter. This vector is then evaluated by the “TRUE %in%” phrase to determine whether the logical TRUE is present. Because this phrase is preceded by ! (and the whole argument is wrapped in parentheses), if TRUE is not present, the file_move() function is engaged and the contract is moved into our OCR folder for further processing.\nTesting shows that the loop is able to detect files without text, and successfully moves them to the desired folder.\nScraping Algorithm\nNow we can put all three elements together into our final algorithm. While testing, we noticed that the OLMS website will time out during the download phase if all files are not pulled within 60 seconds. To overcome this, we used a loop to split the download phase into 27 iterations of ~100 files each. Note that this includes a print() function as a progress heuristic so we can monitor which iteration we are on. Our for/if loop described earlier caps off the algorithm by removing any files without text.\n\n\nShow code\n\n# Create vector of contracts and destinations\nurl <- paste0(\"https://olmsapps.dol.gov/olpdr/GetAttachmentServlet?docId=\", \n              clean_contracts_table$CBA.File)\ndest_file <- paste0(\"/Users/dananestor/DACSS/Text as Data/Contract files/\", \n                    clean_contracts_table$CBA.File, \".pdf\")\n\n# Segment to avoid timing out\ncuts <- cbind(seq(1,2623,100),seq(100,2700,100))\ncuts[27,2] = length(dest_file) #manually setting stop\n\n# Download files\nfor (i in 1:(length(cuts)/2)){\n  print(i)\n  download.file(\n    url[cuts[i,1]:cuts[i,2]], \n    dest_file[cuts[i,1]:cuts[i,2]], \n    method = 'libcurl', \n    quiet = TRUE)\n}\n\n# Test for text and move if OCR needed\nfor (i in 1:length(dest_file)){\n  if(!(TRUE %in%\n        str_detect(\n          pdf_text(dest_file[i]), \n          \"[:alpha:]+\"))){ \n        file_move(\n          dest_file[i], \n          \"/Users/dananestor/DACSS/Text as Data/Contract files/OCR\")}\n}\n\n\n\nResults\nUsing this algorithm, 2,621 contracts were successfully downloaded, totaling 6.75 GB of data. Of those, 371 (984 MB) were not machine readable and require further processing. Two files, CBA numbers 297 and 2212, were too large to download as part of the loop so were pulled down manually. Results were cross validated by examining the local folder and testing a number of randomly-selected files.\nBibliography\nOLMS Online Public Disclosure Room. Retrieved from https://olmsapps.dol.gov/olpdr/?&_ga=2.149185590.1424332960.1649032027-354341244.1643300869#CBA%20Search/CBA%20Search/\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-03T22:29:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-05-text1/",
    "title": "Text as Data Post 1",
    "description": "General outline of my proposed research topic with analysis on feasability and a brief literature review.",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-05",
    "categories": [
      "Text as Data"
    ],
    "contents": "\nResearch Questions\nBackground\nManagement rights clauses are used to strategically reserve certain bargaining positions allowing “exclusive rights to manage, to direct…employees; to evaluate performance, to discipline and discharge employees, to adopt and enforce rules and regulations and policies.” Aided by friendly judicial decisions, these clauses are often beyond reach of the National Labor Relations Board (NLRB) and lower court review.\nBy claiming certain rights and bargaining to impasse, management is able to immediately implement their last best offer at the expiration of a collective bargaining agreement (CBA). Fighting this is a time consuming and expensive proposition, making this a significant point of leverage in contract negotiations.\nAdditionally, courts have relied on empirical analyses of the use of management rights clauses in some of their foundational decisions on this topic (see NLRB v. American National Insurance Co., 343 U.S. 395 (1952)). This opens the door for further analysis to not only improve understanding of the use and spread of these clauses, but even to introduce new evidence that may sway the judiciary towards a new view of the legitimacy of the practice.\nPotential Avenues of Exploration\nMap proliferation of specific clauses: how have these clauses spread? Is there a distinguishable network that we can identify from the data?\nHarness metadata of CBAs to create networks, attach directionality and weights based on identified management rights features to identify probabilities that specific clauses or contract language disseminated via a network or by chance.\n\nQuantify evolution of clauses: how has the language changed? Can we identify specific trends by sector, industry, size of organization, etc.?\nUse statistical analysis (machine learning classification models) to quantify the probability that a clause is related to management rights, combine with content/thematic analysis and/or critical discourse analysis to quantify these changes.\nEstablish supervised scaling approach to identify potential latent dimensions of the text and any correlations to networks, industries, etc.\n\nCompare against history: can we see correlations between changes in language or speed of proliferation against major milestones in the development of labor law and/or union strategy?\nData Sources\nCBA data is available online for contracts dating all the way back to 1935, 12 years before the Taft-Hartley amendment mandated centralized record keeping. The U.S. Department of Labor Office of Labor-Management Standards (DOL-OLMS) maintains mostly current (though also some historical) records both online and as a Microsoft Access database, including metadata on the bargaining parties, contract dates, employee counts, industry, and links to PDF copies of the full CBA. This dataset is inclusive of both public and private sector CBAs. As of February 5th, 2022, the DOL-OLMS database contained 3,730 entries.\nAdditionally, Cornell University’s School of Industrial and Labor Relations, Catherwood Library, maintains a historical database on behalf of DOL-OLMS. This is where most pre-1990 CBAs can be found, and it contains similar information to the DOL-OLMS database in terms of metadata and full CBAs. While Cornell is in the process of fully converting those files into machine-readable format, it is unclear what percentage of the collection is currently in this form - this could present a significant issue in data collection. That said, the data set currently contains 2,834 documents dating back to 1935.\nFinally, the University of California Berkeley’s Institute for Research on Labor and Employment maintains a fully-text-recognized database of union contracts from around the world. However, this data set is the smallest of the three and contains mostly public sector agreements, which differ substantially from the private sector when it comes to management rights clauses due to structural differences in the industry and laws governing these contracts. This database likely will not benefit the project.\nLiterature Review\nAsh, E., MacLeod, W. B., & Naidu, S. (n.d.-a). Optimal Contract Design in the Wild: Rigidity and Control in Collective Bargaining. 46.\nAnalysis of a corpus of 30,000 collective bargaining agreements from Canada from 1986 through 2015. Using ideas and methods from computational linguistics, authors extract measures of rigidity and worker control from the text of the contract clauses. They then analyze how rigidity and authority in contracts varies according to firm-level factors and external factors. This could be used to identify and externally validate the core methodology of this project.\n\nAsh, E., MacLeod, W. B., & Naidu, S. (n.d.-b). The Language of Contract: Promises and Power in Union Collective Bargaining Agreements. 59.\nSame authors and data as previous entry\n\nRosen, S. Z. (n.d.). Marceau, Drafting a Union Contract. Case Western Reserve Law Review, 6.\nThis handbook provides a perspective on the procedure of drafting a union contract in the 1960s. It could be helpful in identifying changes to procedure over time.\n\nWard, M. N. (2004). Contracting participation out of union culture: Patterns of modality and interactional moves in a labour contract settlement / Maurice Norman Ward. [Thesis, The University of Adelaide]. https://digital.library.adelaide.edu.au/dspace/handle/2440/22342\nThis doctoral thesis uses Systemic Functional Linguistics, Critical Discourse Analysis, qualitative, and computational analysis to investigate how language and power interact to construct relationships in the union setting and whether or not union discourse structures promote member participation. While it concentrates on only four documents, the methodologies described here could be useful for latent dimension identification and analysis.\n\n",
    "preview": {},
    "last_modified": "2022-03-10T18:27:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-02-networks2/",
    "title": "Short Assignment 2",
    "description": "Short assignment 2 for Political and Social Network Analysis",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-02-02",
    "categories": [],
    "contents": "\nUtilizing the provided bill cosponsorship data from the 112th congress and (modified) related import scripts, I read-in a CSV file and built both igraph and statnet objects to commence investigation. This data contains unique identifiers for each Member, the bill number, the date the Member joined in cosponsorship (and left, if applicable) and whether or not the member was an original co-sponsor.\n\n\ndata <- read_csv(\"~/DACSS/Network Analysis/govtrack_cosponsor_data_112_congress.csv\")\n\n# Provided script, modified so it would work with the available data\n nodes <- data[c(\"name\",\"thomas_id\",\"bioguide_id\",\"state\",\"district\")]\n  nodes <- distinct(nodes, name, state, bioguide_id, thomas_id, .keep_all = TRUE)\n  \n  #There are repeat entries for congress people who are given both a thomas_id (old system) and a\n  #bioguide_id (new system). Lets fix this by splitting and merging. \n    nodes_a  <- nodes[is.na(nodes$thomas_id),]\n    nodes_a  <- nodes[c(\"name\",\"state\",\"district\",\"bioguide_id\")]\n    nodes_b  <- nodes[is.na(nodes$bioguide_id),]\n    nodes_b  <- nodes[c(\"name\",\"state\",\"district\",\"thomas_id\")]\n    nodes    <- merge(x = nodes_a, y = nodes_b, by = c(\"name\",\"state\",\"district\"), all = TRUE)\n    rm(nodes_a);rm(nodes_b)\n  \n  #Lets also create a new ID that will be assigned to all congress people\n    nodes$ID <- 1:nrow(nodes)\n  \n  #Lets reorder the data putting the ID first\n    nodes <- nodes[c(\"ID\",\"name\",\"state\",\"district\",\"bioguide_id\",\"thomas_id\")]\n  \n#Now let's create a dataframe that contains just edge atributes\n  #Lets add the from_id collumn, replacing all the node attributes given for the senator cosponsoring\n    edge_list <- data\n    edge_list$node_1[!is.na(edge_list$thomas_id)]    <- nodes$ID[match(edge_list$thomas_id, nodes$thomas_id)][!is.na(edge_list$thomas_id)]\n    edge_list$node_1[!is.na(edge_list$bioguide_id)]  <- nodes$ID[match(edge_list$bioguide_id, nodes$bioguide_id)][!is.na(edge_list$bioguide_id)]\n    edge_list <- edge_list[c(\"node_1\",\"bill_number\",\"original_cosponsor\",\"date_signed\",\"date_withdrawn\",\"sponsor\")]\n  \n  #At this point, the \"edges\" dataframe contains links between sponsors and bills. Instead we want want \n  #the edgelist to represent to links between legislators. \n  #Let's do that by replacing the bill number collumn with the ID of the bill's original sponsor\n    sponsor_key    <- edge_list[edge_list$sponsor == TRUE, c(\"node_1\",\"bill_number\")]\n    edge_list$node_2   <- sponsor_key$node_1[match(edge_list$bill_number, sponsor_key$bill_number)]\n    \n  #Lets reorder the dataframe, putting the edgelist in the first two collumns\n      edge_list <- edge_list[c('node_1', 'node_2', 'bill_number','sponsor', 'original_cosponsor', 'date_signed', 'date_withdrawn')]\n    \n  #We dont need to keep the looped connections that represent legislators sponsoring their own bills\n      edge_list <- edge_list[edge_list$sponsor == FALSE,]\n  \n  #We can now remove the sponsor collum\n      edge_list <- edge_list[c('node_1', 'node_2', 'bill_number','original_cosponsor', 'date_signed', 'date_withdrawn')]\n      \n  #And remove unessesary objects\n      rm(sponsor_key)\n\n#Now let's make an igraph object\n  network_igraph <- graph_from_data_frame(d = edge_list, directed = TRUE, vertices = nodes)\n  \n#Now lets create a statnet object\n  \n  network_statnet <- network(as.matrix(edge_list[1:2]), matrix.type = \"edgelist\", directed = TRUE)\n  \n  network_statnet%e%'bill_number'         <- as.character(edge_list$bill_number)\n  network_statnet%e%'original_cosponsor'  <- as.character(edge_list$original_cosponsor)\n  network_statnet%e%'date_signed'         <- as.character(edge_list$date_signed)\n  network_statnet%e%'date_withdrawn'      <- as.character(edge_list$date_withdrawn)\n  \n  network_statnet%v%'name'        <-as.character(nodes$name[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'state'       <-as.character(nodes$state[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'district'    <-as.character(nodes$district[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'bioguide_id' <-as.character(nodes$bioguide_id[match(nodes$ID,network_statnet%v%'vertex.names')])\n  network_statnet%v%'thomas_id'   <-as.character(nodes$thomas_id[match(nodes$ID,network_statnet%v%'vertex.names')])\n  \n\n#Lets create properly named objects and delete unessesary ones\n  network_nodes <- nodes\n  network_edgelist <- edge_list\n  rm(nodes);rm(data);rm(edge_list)\n\n\n\nIn further examining the data, we can see that there are 550 vertices and 1.32863^{5} edges in the igraph network. Additional features include:\nFeature\nT/F?\nBipartite\nFALSE\nDirected\nTRUE\nWeighted\nFALSE\nComparing the igraph object to the statnet object, we can see that the same network features hold true.\nWe can now take a dyad census to get an initial understanding of the connections in our network.\n\n\nigraph::dyad.census(network_igraph)\n\n\n$mut\n[1] 26734\n\n$asym\n[1] 79395\n\n$null\n[1] 44846\n\nsna::dyad.census(network_statnet)\n\n\n       Mut  Asym  Null\n[1,] 16388 35138 99449\n\nFor some reason, the two network objects are returning different measurements with respect to the number of dyad types. This will require further exploration at a later point.\nNext, we examine triads using a census\n\n\nigraph::triad_census(network_igraph)\n\n\n [1] 8427962 5409952 7049451  427375 1236075  731252 1134244  520424\n [9]  673093   36051  245990  472578  271413  243218  507035  191987\n\nsna::triad.census(network_statnet)\n\n\n         003     012     102   021D    021U   021C    111D   111U\n[1,] 8427962 8202667 4256736 427375 1236075 731252 1134244 520424\n       030T  030C    201   120D   120U   120C    210    300\n[1,] 673093 36051 245990 472578 271413 243218 507035 191987\n\nDespite the odd results of the dyad census, the triad census does show identical measurements across the two network object types. To double check that all triads are counted, we can calculate the number of potential triads as (550 * 549 * 548)/6 = 2.75781^{7}. Then, we sum the number of triads in our census sum(igraph::triad_census(network_igraph)) = 2.75781^{7}. The numbers match so we know our function worked correctly.\nFinally, we can look at transitivity within the network to determine the proportion of complete triads in the network.\n\n\ntransitivity(network_igraph)\n\n\n[1] 0.6258881\n\ngtrans(network_statnet)\n\n\n[1] 0.5606059\n\nHere, we see that the transitivity calculatuons differ between the two different network objects. This is likely because the statnet function calculates transitivity slightly differently for directed networks and omits certain triads missing information. However, both functions return a relatively high transitivity score, which makes sense given that this network is specifically intended to involve significant leveraging of connections and wheeling-dealing which requires more transitivity.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-02T21:08:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to my new blog, where I'll be posting assignments from my master's program in data analytics and computational social science",
    "author": [
      {
        "name": "Dana Nestor",
        "url": "https://dnestor.github.io/"
      }
    ],
    "date": "2022-01-29",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-11T00:53:11-05:00",
    "input_file": {}
  }
]
